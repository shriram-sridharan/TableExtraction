<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">

<pdf2xml>
<page number="1" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="0" size="24" family="Times" color="#231f20"/>
	<fontspec id="1" size="15" family="Times" color="#231f20"/>
	<fontspec id="2" size="12" family="Times" color="#231f20"/>
	<fontspec id="3" size="11" family="Times" color="#231f20"/>
	<fontspec id="4" size="9" family="Times" color="#231f20"/>
<text top="108" left="207" width="500" height="26" font="0"><b>Design and Evaluation of Main Memory</b></text>
<text top="138" left="189" width="536" height="26" font="0"><b>Hash Join Algorithms for Multi-core CPUs</b></text>
<text top="206" left="256" width="115" height="17" font="1">Spyros Blanas</text>
<text top="206" left="417" width="64" height="17" font="1">Yinan Li</text>
<text top="206" left="527" width="132" height="17" font="1">Jignesh M. Patel</text>
<text top="224" left="348" width="219" height="14" font="2">University of Wisconsin–Madison</text>
<text top="240" left="306" width="303" height="17" font="1">{sblanas, yinan, jignesh}@cs.wisc.edu</text>
<text top="312" left="81" width="97" height="16" font="1">ABSTRACT</text>
<text top="336" left="81" width="358" height="13" font="3">The focus of this paper is on investigating eﬃcient hash join</text>
<text top="352" left="81" width="359" height="13" font="3">algorithms for modern multi-core processors in main mem-</text>
<text top="367" left="81" width="359" height="13" font="3">ory environments. This paper dissects each internal phase</text>
<text top="383" left="81" width="359" height="13" font="3">of a typical hash join algorithm and considers diﬀerent al-</text>
<text top="399" left="81" width="358" height="13" font="3">ternatives for implementing each phase, producing a family</text>
<text top="414" left="81" width="359" height="13" font="3">of hash join algorithms. Then, we implement these main</text>
<text top="430" left="81" width="359" height="13" font="3">memory algorithms on two radically diﬀerent modern multi-</text>
<text top="446" left="81" width="359" height="13" font="3">processor systems, and carefully examine the factors that</text>
<text top="461" left="81" width="245" height="13" font="3">impact the performance of each method.</text>
<text top="477" left="94" width="345" height="13" font="3">Our analysis reveals some interesting results – a very sim-</text>
<text top="493" left="81" width="359" height="13" font="3">ple hash join algorithm is very competitive to the other</text>
<text top="508" left="81" width="359" height="13" font="3">more complex methods. This simple join algorithm builds a</text>
<text top="524" left="81" width="358" height="13" font="3">shared hash table and does not partition the input relations.</text>
<text top="540" left="81" width="359" height="13" font="3">Its simplicity implies that it requires fewer parameter set-</text>
<text top="555" left="81" width="359" height="13" font="3">tings, thereby making it far easier for query optimizers and</text>
<text top="571" left="81" width="359" height="13" font="3">execution engines to use it in practice. Furthermore, the</text>
<text top="587" left="81" width="358" height="13" font="3">performance of this simple algorithm improves dramatically</text>
<text top="603" left="81" width="359" height="13" font="3">as the skew in the input data increases, and it quickly starts</text>
<text top="618" left="81" width="359" height="13" font="3">to outperform all other algorithms. Based on our results,</text>
<text top="634" left="81" width="359" height="13" font="3">we propose that database implementers consider adding this</text>
<text top="650" left="81" width="359" height="13" font="3">simple join algorithm to their repertoire of main memory</text>
<text top="665" left="81" width="359" height="13" font="3">join algorithms, or adapt their methods to mimic the strat-</text>
<text top="681" left="81" width="359" height="13" font="3">egy employed by this algorithm, especially when joining in-</text>
<text top="697" left="81" width="220" height="13" font="3">puts with skewed data distributions.</text>
<text top="731" left="81" width="270" height="16" font="1">Categories and Subject Descriptors</text>
<text top="755" left="81" width="359" height="13" font="3">H.2.4. [Database Management]: Systems—Query pro-</text>
<text top="771" left="81" width="172" height="12" font="3">cessing, Relational databases</text>
<text top="805" left="81" width="114" height="16" font="1">General Terms</text>
<text top="829" left="81" width="200" height="13" font="3">Algorithms, Design, Performance</text>
<text top="863" left="81" width="77" height="16" font="1">Keywords</text>
<text top="887" left="81" width="216" height="13" font="3">hash join, multi-core, main memory</text>
<text top="976" left="81" width="359" height="11" font="4">Permission to make digital or hard copies of all or part of this work for</text>
<text top="990" left="81" width="359" height="11" font="4">personal or classroom use is granted without fee provided that copies are</text>
<text top="1003" left="81" width="358" height="11" font="4">not made or distributed for proﬁt or commercial advantage and that copies</text>
<text top="1017" left="81" width="359" height="11" font="4">bear this notice and the full citation on the ﬁrst page. To copy otherwise, to</text>
<text top="1030" left="81" width="358" height="11" font="4">republish, to post on servers or to redistribute to lists, requires prior speciﬁc</text>
<text top="1044" left="81" width="115" height="11" font="4">permission and/or a fee.</text>
<text top="1058" left="81" width="65" height="11" font="4">SIGMOD’11,</text>
<text top="1057" left="149" width="167" height="11" font="4">June 12–16, 2011, Athens, Greece.</text>
<text top="1071" left="81" width="280" height="11" font="4">Copyright 2011 ACM 978-1-4503-0661-4/11/06 ...$10.00.</text>
<text top="312" left="475" width="13" height="16" font="1">1.</text>
<text top="312" left="507" width="143" height="16" font="1">INTRODUCTION</text>
<text top="334" left="489" width="345" height="13" font="3">Large scale multi-core processors are imminent. Modern</text>
<text top="349" left="475" width="359" height="13" font="3">processors today already have four or more cores, and for the</text>
<text top="365" left="475" width="359" height="13" font="3">past few years Intel has been introducing two more cores</text>
<text top="381" left="475" width="359" height="13" font="3">per processor roughly every 15 months. At this rate, it</text>
<text top="396" left="475" width="359" height="13" font="3">is not hard to imagine running database management sys-</text>
<text top="412" left="475" width="359" height="13" font="3">tems (DBMSs) on processors with hundreds of cores in the</text>
<text top="428" left="475" width="359" height="13" font="3">near future. In addition, memory prices are continuing to</text>
<text top="444" left="475" width="359" height="13" font="3">drop. Today 1TB of memory costs as little as $25,000. Con-</text>
<text top="459" left="475" width="358" height="13" font="3">sequently, many databases now either ﬁt entirely in main</text>
<text top="475" left="475" width="359" height="13" font="3">memory, or their working set is main memory resident. As</text>
<text top="490" left="475" width="303" height="13" font="3">a result, many DBMSs are becoming CPU bound.</text>
<text top="506" left="489" width="345" height="13" font="3">In this evolving architectural landscape, DBMSs have the</text>
<text top="522" left="475" width="359" height="13" font="3">unique opportunity to leverage the inherent parallelism that</text>
<text top="538" left="475" width="359" height="13" font="3">is provided by the relational data model. Data is exposed</text>
<text top="553" left="475" width="359" height="13" font="3">by declarative query languages to user applications and the</text>
<text top="569" left="475" width="359" height="13" font="3">DBMS is free to choose its execution strategy. Coupled</text>
<text top="585" left="475" width="359" height="13" font="3">with the trend towards impending very large multi-cores,</text>
<text top="600" left="475" width="359" height="13" font="3">this implies that DBMSs must carefully rethink how they</text>
<text top="616" left="475" width="359" height="13" font="3">can exploit the parallelism that is provided by the modern</text>
<text top="632" left="475" width="332" height="13" font="3">multi-core processors, or DBMS performance will stall.</text>
<text top="647" left="489" width="345" height="13" font="3">A natural question to ask then is whether there is anything</text>
<text top="663" left="475" width="359" height="13" font="3">new here. Beginning about three decades ago, at the incep-</text>
<text top="679" left="475" width="359" height="13" font="3">tion of the ﬁeld of parallel DBMSs, the database community</text>
<text top="694" left="475" width="359" height="13" font="3">thoroughly examined how a DBMS can use various forms of</text>
<text top="710" left="475" width="359" height="13" font="3">parallelism. These forms of parallelism include pure shared-</text>
<text top="726" left="475" width="358" height="13" font="3">nothing, shared-memory, and shared disk architectures [17].</text>
<text top="742" left="475" width="359" height="13" font="3">If the modern multi-core architectures resemble any of these</text>
<text top="757" left="475" width="359" height="13" font="3">architectural templates, then we can simply adopt the meth-</text>
<text top="773" left="475" width="223" height="13" font="3">ods that have already been designed.</text>
<text top="789" left="489" width="345" height="13" font="3">In fact, to a large extent this is the approach that DBMSs</text>
<text top="804" left="475" width="359" height="13" font="3">have haven taken towards dealing with multi-core machines.</text>
<text top="820" left="475" width="359" height="13" font="3">Many commercial DBMSs simply treat a multi-core proces-</text>
<text top="836" left="475" width="359" height="13" font="3">sor as a symmetric multi-processor (SMP) machine, lever-</text>
<text top="851" left="475" width="359" height="13" font="3">aging previous work that was done by the DBMS vendors</text>
<text top="867" left="475" width="359" height="13" font="3">in reaction to the increasing popularity of SMP machines</text>
<text top="883" left="475" width="359" height="13" font="3">decades ago. These methods break up the task of a single</text>
<text top="899" left="475" width="358" height="13" font="3">operation, such as an equijoin, into disjoint parts and allow</text>
<text top="914" left="475" width="359" height="13" font="3">each processor (in an SMP box) to work on each part in-</text>
<text top="930" left="475" width="359" height="13" font="3">dependently. At a high-level, these methods resemble vari-</text>
<text top="946" left="475" width="359" height="13" font="3">ations of query processing techniques that were developed</text>
<text top="961" left="475" width="359" height="13" font="3">for parallel shared-nothing architectures [6], but adapted</text>
<text top="977" left="475" width="359" height="13" font="3">for SMP machines. In most commercial DBMSs, this ap-</text>
<text top="993" left="475" width="359" height="13" font="3">proach is reﬂected across the entire design process, ranging</text>
<text top="1008" left="475" width="359" height="13" font="3">from system internals (join processing, for example) to their</text>
<text top="1024" left="475" width="359" height="13" font="3">pricing model, which is frequently done by scaling the SMP</text>
<text top="1040" left="475" width="359" height="13" font="3">pricing model. On the other hand, open-source DBMSs have</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="1188" width="918">
<text top="85" left="81" width="359" height="13" font="3">largely ignored multi-core processing and generally dedicate</text>
<text top="101" left="81" width="229" height="13" font="3">a single thread/process to each query.</text>
<text top="117" left="94" width="345" height="13" font="3">The design space for modern high performance main mem-</text>
<text top="133" left="81" width="359" height="13" font="3">ory join algorithms has two extremes. One extreme of this</text>
<text top="148" left="81" width="359" height="13" font="3">design space focuses on minimizing the number of proces-</text>
<text top="164" left="81" width="359" height="13" font="3">sor cache misses. The radix-based hash join algorithm [2] is</text>
<text top="180" left="81" width="359" height="13" font="3">an example of a method in this design class. The other ex-</text>
<text top="195" left="81" width="359" height="13" font="3">treme is to focus on minimizing processor synchronization</text>
<text top="211" left="81" width="359" height="13" font="3">costs. In this paper we propose a “no partitioning” hash</text>
<text top="227" left="81" width="359" height="13" font="3">join algorithm that does not partition the input relations to</text>
<text top="242" left="81" width="356" height="13" font="3">embody an example of a method in this later design space.</text>
<text top="258" left="94" width="345" height="13" font="3">A crucial question that we ask and answer in this paper</text>
<text top="274" left="81" width="359" height="13" font="3">is what is the impact of these two extreme design points in</text>
<text top="290" left="81" width="359" height="12" font="3">modern multi-core processors for main memory hash join al-</text>
<text top="306" left="81" width="52" height="12" font="3">gorithms</text>
<text top="305" left="132" width="307" height="13" font="3">. A perhaps surprising answer is that for modern</text>
<text top="321" left="81" width="359" height="13" font="3">multi-core architectures, in many cases the right approach is</text>
<text top="337" left="81" width="359" height="13" font="3">to focus on reducing the computation and synchronization</text>
<text top="352" left="81" width="359" height="13" font="3">costs, as modern processors are very eﬀective in hiding cache</text>
<text top="368" left="81" width="359" height="13" font="3">miss latencies via simultaneous multi-threading. For exam-</text>
<text top="384" left="81" width="358" height="13" font="3">ple, in our experiments, the “no partitioning” hash join algo-</text>
<text top="399" left="81" width="359" height="13" font="3">rithm far outperforms the radix join algorithm when there</text>
<text top="415" left="81" width="359" height="13" font="3">is skew in the data (which is often the case in practice), even</text>
<text top="431" left="81" width="358" height="13" font="3">while it incurs many more processor cache and TLB misses.</text>
<text top="446" left="81" width="359" height="13" font="3">Even with uniform data, the radix join algorithm only out-</text>
<text top="462" left="81" width="359" height="13" font="3">performs the “no partitioning” algorithm on a modern Intel</text>
<text top="478" left="81" width="359" height="13" font="3">Xeon when the parameters for the radix join algorithm are</text>
<text top="494" left="81" width="359" height="13" font="3">set at or near their optimal setting. In contrast, the non-</text>
<text top="509" left="81" width="359" height="13" font="3">partitioned algorithm is “parameter-free”, which is another</text>
<text top="525" left="81" width="187" height="13" font="3">important practical advantage.</text>
<text top="541" left="94" width="345" height="13" font="3">Reﬂecting on the previous work in this area, one can ob-</text>
<text top="556" left="81" width="359" height="13" font="3">serve that the database community has focused on optimiz-</text>
<text top="572" left="81" width="359" height="13" font="3">ing query processing methods to reduce the number of pro-</text>
<text top="588" left="81" width="359" height="13" font="3">cessor cache and TLB misses. We hope that this paper opens</text>
<text top="603" left="81" width="359" height="13" font="3">up a new discussion on the entire design space for multi-core</text>
<text top="619" left="81" width="359" height="13" font="3">query processing techniques, and incites a similar examina-</text>
<text top="635" left="81" width="359" height="13" font="3">tion of other aspects of query processing beyond the single</text>
<text top="651" left="81" width="299" height="13" font="3">hash join operation that we discuss in this paper.</text>
<text top="666" left="94" width="345" height="13" font="3">This paper makes three main contributions. First, we sys-</text>
<text top="682" left="81" width="359" height="13" font="3">tematically examine the design choices available for each in-</text>
<text top="697" left="81" width="359" height="13" font="3">ternal phase of a canonical main memory hash join algorithm</text>
<text top="713" left="81" width="359" height="13" font="3">– namely, the partition, build, and probe phases – and enu-</text>
<text top="729" left="81" width="359" height="13" font="3">merate a number of possible multi-core hash join algorithms</text>
<text top="745" left="81" width="359" height="13" font="3">based on diﬀerent choices made in each of these phases. We</text>
<text top="760" left="81" width="359" height="13" font="3">then evaluate these join algorithms on two radically diﬀer-</text>
<text top="776" left="81" width="359" height="13" font="3">ent architectures and show how the architectural diﬀerences</text>
<text top="792" left="81" width="359" height="13" font="3">can aﬀect performance. Unlike previous work that has often</text>
<text top="807" left="81" width="359" height="13" font="3">focused on just one architecture, our use of two radically dif-</text>
<text top="823" left="81" width="359" height="13" font="3">ferent architectures lets us gain deeper insights about hash</text>
<text top="839" left="81" width="359" height="13" font="3">join processing on multi-core processors. To the best of our</text>
<text top="854" left="81" width="359" height="13" font="3">knowledge, this is the ﬁrst systematic exploration of multiple</text>
<text top="870" left="81" width="342" height="13" font="3">hash join techniques that spans multi-core architectures.</text>
<text top="886" left="94" width="345" height="13" font="3">Second, we show that an algorithm that does not do any</text>
<text top="901" left="81" width="359" height="13" font="3">partitioning, but simply constructs a single shared hash ta-</text>
<text top="917" left="81" width="359" height="13" font="3">ble on the build relation often outperforms more complex al-</text>
<text top="933" left="81" width="359" height="13" font="3">gorithms. This simple “no-partitioning” hash join algorithm</text>
<text top="949" left="81" width="359" height="13" font="3">is robust to sub-optimal parameter choices by the optimizer,</text>
<text top="964" left="81" width="359" height="13" font="3">and does not require any knowledge of the characteristics of</text>
<text top="980" left="81" width="359" height="13" font="3">the input to work well. To the best of our knowledge, this</text>
<text top="996" left="81" width="359" height="13" font="3">simple hash join technique diﬀers from what is currently</text>
<text top="1011" left="81" width="358" height="13" font="3">implemented in existing DBMSs for multi-core hash join</text>
<text top="1027" left="81" width="359" height="13" font="3">processing, and oﬀers a tantalizingly simple, eﬃcient, and</text>
<text top="1043" left="81" width="357" height="13" font="3">robust technique for implementing the hash join operation.</text>
<text top="85" left="489" width="345" height="13" font="3">Finally, we show that the simple “no-partitioning” hash</text>
<text top="101" left="475" width="359" height="13" font="3">join algorithm takes advantage of intrinsic hardware opti-</text>
<text top="117" left="475" width="358" height="13" font="3">mizations to handle skew. As a result, this simple hash join</text>
<text top="133" left="475" width="358" height="13" font="3">technique often beneﬁts from skew and its relative perfor-</text>
<text top="148" left="475" width="359" height="13" font="3">mance increases as the skew increases! This property is a</text>
<text top="164" left="475" width="359" height="13" font="3">big advancement over the state-of-the-art methods, as it is</text>
<text top="180" left="475" width="359" height="13" font="3">important to have methods that can gracefully handle skew</text>
<text top="195" left="475" width="86" height="13" font="3">in practice [8].</text>
<text top="211" left="489" width="345" height="13" font="3">The remainder of this paper is organized as follows: The</text>
<text top="227" left="475" width="358" height="13" font="3">next section covers background information. The hash join</text>
<text top="242" left="475" width="359" height="13" font="3">variants are presented in Section 3. Experimental results are</text>
<text top="258" left="475" width="359" height="13" font="3">described in Section 4, and related work is discussed in Sec-</text>
<text top="274" left="475" width="354" height="13" font="3">tion 5. Finally, Section 6 contains our concluding remarks.</text>
<text top="295" left="475" width="13" height="16" font="1">2.</text>
<text top="295" left="507" width="273" height="16" font="1">THE MULTI-CORE LANDSCAPE</text>
<text top="317" left="489" width="345" height="13" font="3">In the last few years alone, more than a dozen diﬀerent</text>
<text top="332" left="475" width="359" height="13" font="3">multi-core CPU families have been introduced by CPU ven-</text>
<text top="348" left="475" width="359" height="13" font="3">dors. These new CPUs have ranged from powerful dual-CPU</text>
<text top="364" left="475" width="359" height="13" font="3">systems on the same die to prototype systems of hundreds</text>
<text top="380" left="475" width="130" height="13" font="3">of simple RISC cores.</text>
<text top="395" left="489" width="345" height="13" font="3">This new level of integration has lead to architectural</text>
<text top="411" left="475" width="359" height="13" font="3">changes with deep impact on algorithm design. Although</text>
<text top="427" left="475" width="359" height="13" font="3">the ﬁrst multi-core CPUs had dedicated caches for each core,</text>
<text top="442" left="475" width="359" height="13" font="3">we now see a shift towards more sharing at the lower levels</text>
<text top="458" left="475" width="358" height="13" font="3">of the cache hierarchy and consequently the need for access</text>
<text top="474" left="475" width="359" height="13" font="3">arbitration to shared caches within the chip. A shared cache</text>
<text top="489" left="475" width="359" height="13" font="3">means better single-threaded performance, as one core can</text>
<text top="505" left="475" width="359" height="13" font="3">utilize the whole cache, and more opportunities for sharing</text>
<text top="521" left="475" width="359" height="13" font="3">among cores. However, shared caches also increase conﬂict</text>
<text top="537" left="475" width="359" height="13" font="3">cache misses due to false sharing, and may increase capacity</text>
<text top="552" left="475" width="358" height="13" font="3">cache misses, if the cache sizes don’t increase proportionally</text>
<text top="568" left="475" width="140" height="13" font="3">to the number of cores.</text>
<text top="584" left="489" width="345" height="13" font="3">One idea that is employed to combat the diminishing re-</text>
<text top="599" left="475" width="359" height="13" font="3">turns of instruction-level parallelism is simultaneous multi-</text>
<text top="615" left="475" width="359" height="13" font="3">threading (SMT). Multi-threading attempts to ﬁnd inde-</text>
<text top="631" left="475" width="359" height="13" font="3">pendent instructions across diﬀerent threads of execution,</text>
<text top="646" left="475" width="359" height="13" font="3">instead of detecting independent instructions in the same</text>
<text top="662" left="475" width="359" height="13" font="3">thread. This way, the CPU will schedule instructions from</text>
<text top="678" left="475" width="359" height="13" font="3">each thread and achieve better overall utilization, increasing</text>
<text top="693" left="475" width="295" height="13" font="3">throughput at the expense of per-thread latency.</text>
<text top="709" left="489" width="345" height="13" font="3">We brieﬂy consider two modern architectures that we sub-</text>
<text top="725" left="475" width="359" height="13" font="3">sequently use for evaluation. At one end of the spectrum,</text>
<text top="741" left="475" width="359" height="13" font="3">the Intel Nehalem family is an instance of Intel’s latest mi-</text>
<text top="756" left="475" width="359" height="13" font="3">croarchitecture that oﬀers high single-threaded performance</text>
<text top="772" left="475" width="358" height="13" font="3">because of its out-of-order execution and on-demand fre-</text>
<text top="787" left="475" width="359" height="13" font="3">quency scaling (TurboBoost). Multi-threaded performance</text>
<text top="803" left="475" width="359" height="13" font="3">is increased by using simultaneous multi-threading (Hyper-</text>
<text top="819" left="475" width="359" height="13" font="3">Threading). At the other end of the spectrum, the Sun</text>
<text top="835" left="475" width="359" height="13" font="3">UltraSPARC T2 has 8 simple cores that all share a sin-</text>
<text top="850" left="475" width="359" height="13" font="3">gle cache. This CPU can execute instructions from up to</text>
<text top="866" left="475" width="359" height="13" font="3">8 threads per core, or a total of 64 threads for the entire</text>
<text top="882" left="475" width="359" height="13" font="3">chip, and extensively relies on simultaneous multi-threading</text>
<text top="897" left="475" width="202" height="13" font="3">to achieve maximum throughput.</text>
<text top="919" left="475" width="13" height="16" font="1">3.</text>
<text top="919" left="507" width="271" height="16" font="1">HASH JOIN IMPLEMENTATION</text>
<text top="940" left="489" width="345" height="13" font="3">In this section, we consider the anatomy of a canoni-</text>
<text top="956" left="475" width="358" height="13" font="3">cal hash join algorithm, and carefully consider the design</text>
<text top="972" left="475" width="359" height="13" font="3">choices that are available in each internal phase of a hash</text>
<text top="987" left="475" width="359" height="13" font="3">join algorithm. Then using these design choices, we cat-</text>
<text top="1003" left="475" width="358" height="13" font="3">egorize various previous proposals for multi-core hash join</text>
<text top="1019" left="475" width="358" height="13" font="3">processing. In the following discussion we also present infor-</text>
<text top="1034" left="475" width="359" height="13" font="3">mation about some of the implementation details, as they</text>
<text top="1050" left="475" width="359" height="13" font="3">often have a signiﬁcant impact on the performance of the</text>
<text top="1066" left="475" width="166" height="13" font="3">technique that is described.</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="5" size="6" family="Times" color="#231f20"/>
	<fontspec id="6" size="5" family="Times" color="#231f20"/>
	<fontspec id="7" size="14" family="Times" color="#231f20"/>
<text top="85" left="94" width="345" height="13" font="3">A hash join operator works on two input relations, R and</text>
<text top="102" left="81" width="8" height="12" font="3">S</text>
<text top="101" left="90" width="349" height="13" font="3">. We assume that |R| &lt; |S|. A typical hash join algorithm</text>
<text top="117" left="81" width="359" height="13" font="3">has three phases: partition, build, and probe. The partition</text>
<text top="133" left="81" width="359" height="13" font="3">phase is optional and divides tuples into distinct sets using</text>
<text top="148" left="81" width="359" height="13" font="3">a hash function on the join key attribute. The build phase</text>
<text top="164" left="81" width="359" height="13" font="3">scans the relation R and creates an in-memory hash table on</text>
<text top="180" left="81" width="358" height="13" font="3">the join key attribute. The probe phase scans the relation</text>
<text top="196" left="81" width="8" height="12" font="3">S</text>
<text top="195" left="90" width="349" height="13" font="3">, looks up the join key of each tuple in the hash table, and</text>
<text top="211" left="81" width="304" height="13" font="3">in the case of a match creates the output tuple(s).</text>
<text top="227" left="94" width="345" height="13" font="3">Before we discuss the alternative techniques that are avail-</text>
<text top="242" left="81" width="359" height="13" font="3">able in each phase of the join algorithm, we brieﬂy digress</text>
<text top="258" left="81" width="359" height="13" font="3">to discuss the impact of the latch implementation on the</text>
<text top="274" left="81" width="359" height="13" font="3">join techniques. As a general comment, we have found that</text>
<text top="290" left="81" width="359" height="13" font="3">the latch implementation has a crucial impact on the over-</text>
<text top="305" left="81" width="359" height="13" font="3">all join performance. In particular, when using the pthreads</text>
<text top="321" left="81" width="359" height="13" font="3">mutex implementation, several instructions are required to</text>
<text top="337" left="81" width="359" height="13" font="3">acquire and release an uncontended latch. If there are mil-</text>
<text top="352" left="81" width="359" height="13" font="3">lions of buckets in a hash table, then the hash collision rate</text>
<text top="368" left="81" width="359" height="13" font="3">is small, and one can optimize for the expected case: latches</text>
<text top="384" left="81" width="359" height="13" font="3">being free. Furthermore, pthread mutexes have signiﬁcant</text>
<text top="399" left="81" width="359" height="13" font="3">memory footprint as each requires approximately 40 bytes.</text>
<text top="415" left="81" width="359" height="13" font="3">If each bucket stores a few &lt;key, record-id&gt; pairs, then the</text>
<text top="431" left="81" width="359" height="13" font="3">size of the latch array may be greater than the size of the</text>
<text top="446" left="81" width="359" height="13" font="3">hash table itself. These characteristics make mutexes a pro-</text>
<text top="462" left="81" width="359" height="13" font="3">hibitively expensive synchronization primitive for buckets</text>
<text top="478" left="81" width="359" height="13" font="3">in a hash table. Hence, we implemented our own 1-byte</text>
<text top="494" left="81" width="359" height="13" font="3">latch for both the Intel and the Sun architectures, using the</text>
<text top="509" left="81" width="359" height="13" font="3">atomic primitives xchgb and ldstub, respectively. Protect-</text>
<text top="525" left="81" width="359" height="13" font="3">ing multiple hash buckets with a single latch to avoid cache</text>
<text top="541" left="81" width="359" height="13" font="3">thrashing did not result in signiﬁcant performance improve-</text>
<text top="556" left="81" width="317" height="13" font="3">ments even when the number of partitions was high.</text>
<text top="589" left="81" width="22" height="16" font="1">3.1</text>
<text top="589" left="121" width="117" height="16" font="1">Partition phase</text>
<text top="611" left="94" width="345" height="13" font="3">The partition phase is an optional step of a hash join al-</text>
<text top="627" left="81" width="359" height="13" font="3">gorithm, if the hash table for the relation R ﬁts in main</text>
<text top="642" left="81" width="359" height="13" font="3">memory. If one partitions both the R and S relations such</text>
<text top="658" left="81" width="359" height="13" font="3">that each partition ﬁts in the CPU cache, then the cache</text>
<text top="674" left="81" width="359" height="13" font="3">misses that are otherwise incurred during the subsequent</text>
<text top="689" left="81" width="359" height="13" font="3">build and probe phases are almost eliminated. The cost</text>
<text top="705" left="81" width="359" height="13" font="3">for partitioning both input relations is incurring additional</text>
<text top="721" left="81" width="359" height="13" font="3">memory writes for each tuple. Work by Shatdal et al. [16]</text>
<text top="736" left="81" width="359" height="13" font="3">has shown that the runtime cost of the additional memory</text>
<text top="752" left="81" width="359" height="13" font="3">writes during partitioning phase is less than the cost of miss-</text>
<text top="768" left="81" width="359" height="13" font="3">ing in the cache – as a consequence partitioning improves</text>
<text top="783" left="81" width="359" height="13" font="3">overall performance. Recent work by Cieslewicz and Ross</text>
<text top="799" left="81" width="359" height="13" font="3">[4] has explored partitioning performance in detail. They</text>
<text top="815" left="81" width="359" height="13" font="3">introduce two algorithms that process the input once in a</text>
<text top="831" left="81" width="359" height="13" font="3">serial fashion and do not require any kind of global knowl-</text>
<text top="846" left="81" width="359" height="13" font="3">edge about the characteristics of the input. Another recent</text>
<text top="862" left="81" width="359" height="13" font="3">paper [11] describes a parallel implementation of radix par-</text>
<text top="878" left="81" width="359" height="13" font="3">titioning [2] which gives impressive performance improve-</text>
<text top="893" left="81" width="359" height="13" font="3">ments on a modern multi-core system. This implementation</text>
<text top="909" left="81" width="359" height="13" font="3">requires that the entire input is available upfront and will</text>
<text top="925" left="81" width="359" height="13" font="3">not produce any output until the last input tuple has been</text>
<text top="940" left="81" width="359" height="13" font="3">seen. We experiment with all of these three partitioning al-</text>
<text top="956" left="81" width="359" height="13" font="3">gorithms, and we brieﬂy summarize each implementation in</text>
<text top="972" left="81" width="145" height="13" font="3">Sections 3.1.1 and 3.1.2.</text>
<text top="987" left="94" width="345" height="13" font="3">In our implementation, a partition is a linked list of output</text>
<text top="1003" left="81" width="359" height="13" font="3">buﬀers. An output buﬀer is fully described by four elements:</text>
<text top="1019" left="81" width="359" height="13" font="3">an integer specifying the size of the data block, a pointer to</text>
<text top="1034" left="81" width="359" height="13" font="3">the start of the data block, a pointer to the free space inside</text>
<text top="1050" left="81" width="359" height="13" font="3">the data block and a pointer to the next output buﬀer that</text>
<text top="1066" left="81" width="359" height="13" font="3">is initially set to zero. If a buﬀer overﬂows, then we add an</text>
<text top="85" left="475" width="359" height="13" font="3">empty output buﬀer at the start of the list, and we make its</text>
<text top="101" left="475" width="359" height="13" font="3">next pointer point to the buﬀer that overﬂowed. Locating</text>
<text top="117" left="475" width="359" height="13" font="3">free space is a matter of checking the ﬁrst buﬀer in the list.</text>
<text top="133" left="489" width="345" height="13" font="3">Let p denote the desired number of partitions and n de-</text>
<text top="148" left="475" width="358" height="13" font="3">note the number of threads that are processing the hash join</text>
<text top="164" left="475" width="359" height="13" font="3">operation. During the partitioning phase, all threads start</text>
<text top="180" left="475" width="359" height="13" font="3">reading tuples from the relation R, via a cursor. Each thread</text>
<text top="195" left="475" width="359" height="13" font="3">works on a large batch of tuples at a time, so as to minimize</text>
<text top="211" left="475" width="359" height="13" font="3">synchronization overheads on the input scan cursor. Each</text>
<text top="227" left="475" width="359" height="13" font="3">thread examines a tuple, then extracts the key k, and ﬁ-</text>
<text top="242" left="475" width="290" height="13" font="3">nally computes the partitioning hash function h</text>
<text top="248" left="766" width="6" height="8" font="5">p</text>
<text top="242" left="772" width="62" height="13" font="3">(k). Next,</text>
<text top="258" left="475" width="230" height="13" font="3">it then writes the tuple to partition R</text>
<text top="264" left="705" width="7" height="8" font="5">h</text>
<text top="268" left="711" width="6" height="5" font="6">p</text>
<text top="264" left="718" width="15" height="9" font="5">(k)</text>
<text top="258" left="738" width="96" height="13" font="3">using one of the</text>
<text top="274" left="475" width="359" height="13" font="3">algorithms we describe below. When the R cursor runs out</text>
<text top="290" left="475" width="359" height="13" font="3">of tuples, the partitioning operation proceeds to process the</text>
<text top="305" left="475" width="359" height="13" font="3">tuples from the S relation. Again, each tuple is examined,</text>
<text top="321" left="475" width="359" height="13" font="3">the join key k is extracted and the tuple is written to the</text>
<text top="337" left="475" width="67" height="13" font="3">partition S</text>
<text top="343" left="542" width="7" height="8" font="5">h</text>
<text top="347" left="548" width="6" height="5" font="6">p</text>
<text top="342" left="555" width="15" height="9" font="5">(k)</text>
<text top="337" left="570" width="264" height="13" font="3">. The partitioning phase ends when all the</text>
<text top="353" left="475" width="8" height="12" font="3">S</text>
<text top="352" left="489" width="176" height="13" font="3">tuples have been partitioned.</text>
<text top="368" left="489" width="345" height="13" font="3">Note that we classify the partitioning algorithms as “non-</text>
<text top="384" left="475" width="359" height="13" font="3">blocking” if they produce results on-the-ﬂy and scan the in-</text>
<text top="399" left="475" width="359" height="13" font="3">put once, in contrast to a “blocking” algorithm that produces</text>
<text top="415" left="475" width="359" height="13" font="3">results after buﬀering the entire input and scanning it more</text>
<text top="431" left="475" width="359" height="13" font="3">than once. We acknowledge that the join operator overall</text>
<text top="446" left="475" width="359" height="13" font="3">is never truly non-blocking, as it will block during the build</text>
<text top="462" left="475" width="358" height="13" font="3">phase. The distinction is that the non-blocking algorithms</text>
<text top="478" left="475" width="359" height="13" font="3">only block for the time that is needed to scan and process</text>
<text top="494" left="475" width="359" height="13" font="3">the smaller input, and, as we will see in Section 4.3, this a</text>
<text top="509" left="475" width="259" height="13" font="3">very small fraction of the overall join time.</text>
<text top="539" left="479" width="33" height="15" font="7">3.1.1</text>
<text top="539" left="529" width="165" height="15" font="7">Non-blocking algorithms</text>
<text top="560" left="489" width="345" height="13" font="3">The ﬁrst partitioning algorithm creates p shared partitions</text>
<text top="576" left="475" width="359" height="13" font="3">among all the threads. The threads need to synchronize via</text>
<text top="591" left="475" width="359" height="13" font="3">a latch to make sure that the writes to a shared partition</text>
<text top="607" left="475" width="173" height="13" font="3">are isolated from each other.</text>
<text top="623" left="489" width="345" height="13" font="3">The second partitioning algorithm creates p ∗ n partitions</text>
<text top="638" left="475" width="359" height="13" font="3">in total and each thread is assigned a private set of p parti-</text>
<text top="654" left="475" width="359" height="13" font="3">tions. Each thread then writes to its local partitions without</text>
<text top="670" left="475" width="359" height="13" font="3">any synchronization overhead. When the input relation is</text>
<text top="686" left="475" width="359" height="13" font="3">depleted, all threads synchronize at a barrier to consolidate</text>
<text top="701" left="475" width="221" height="13" font="3">the p ∗ n partitions into p partitions.</text>
<text top="717" left="489" width="345" height="13" font="3">The beneﬁt of creating private partitions is that there is</text>
<text top="733" left="475" width="359" height="13" font="3">no synchronization overhead on each access. The drawbacks,</text>
<text top="748" left="475" width="359" height="13" font="3">however, are (a) many partitions are created, possibly so</text>
<text top="764" left="475" width="359" height="13" font="3">many that the working set of the algorithm no longer ﬁts in</text>
<text top="780" left="475" width="359" height="13" font="3">the data cache and the TLB; (b) at the end of the partition</text>
<text top="795" left="475" width="359" height="13" font="3">phase some thread has to chain n private partitions together</text>
<text top="811" left="475" width="359" height="13" font="3">to form a single partition, but this operation is quick and</text>
<text top="827" left="475" width="116" height="13" font="3">can be parallelized.</text>
<text top="857" left="479" width="33" height="15" font="7">3.1.2</text>
<text top="857" left="529" width="127" height="15" font="7">Blocking algorithm</text>
<text top="878" left="489" width="345" height="13" font="3">Another partitioning technique is the parallel multi-pass</text>
<text top="893" left="475" width="358" height="13" font="3">radix partitioning algorithm described by Kim et al. [11].</text>
<text top="909" left="475" width="359" height="13" font="3">The algorithm begins by having the entire input available in</text>
<text top="925" left="475" width="359" height="13" font="3">a contiguous block of memory. Each thread is responsible</text>
<text top="940" left="475" width="359" height="13" font="3">for a speciﬁc memory region in that contiguous block. A</text>
<text top="956" left="475" width="359" height="13" font="3">histogram with p ∗ n bins is allocated and the input is then</text>
<text top="972" left="475" width="359" height="13" font="3">scanned twice. During the ﬁrst scan, each thread scans all</text>
<text top="987" left="475" width="359" height="13" font="3">the tuples in the memory region assigned to it, extracts the</text>
<text top="1003" left="475" width="359" height="13" font="3">key k and then computes the exact histogram of the hash</text>
<text top="1019" left="475" width="49" height="13" font="3">values h</text>
<text top="1024" left="524" width="6" height="8" font="5">p</text>
<text top="1019" left="531" width="303" height="13" font="3">(k) for this region. Thread i ∈ [0, n − 1] stores the</text>
<text top="1034" left="475" width="359" height="13" font="3">number of tuples it encountered that will hash to partition</text>
<text top="1051" left="475" width="6" height="12" font="3">j</text>
<text top="1050" left="485" width="348" height="13" font="3">∈ [0, p−1] in histogram bin j ∗n+i. At the end of the scan,</text>
<text top="1066" left="475" width="359" height="13" font="3">all the n threads compute the preﬁx sum on the histogram</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="1188" width="918">
<text top="85" left="81" width="359" height="13" font="3">in parallel. The preﬁx sum can now be used to point to the</text>
<text top="101" left="81" width="359" height="13" font="3">beginning of each output partition for each thread in the</text>
<text top="117" left="81" width="359" height="13" font="3">single shared output buﬀer. Finally, each thread performs</text>
<text top="133" left="81" width="270" height="13" font="3">a second scan of its input region, and uses h</text>
<text top="138" left="351" width="6" height="8" font="5">p</text>
<text top="133" left="362" width="77" height="13" font="3">to determine</text>
<text top="148" left="81" width="359" height="13" font="3">the output partition. This algorithm is recursively applied</text>
<text top="164" left="81" width="346" height="13" font="3">to each output partition for as many passes as requested.</text>
<text top="180" left="94" width="345" height="13" font="3">The beneﬁt of radix partitioning is that it makes few cache</text>
<text top="195" left="81" width="359" height="13" font="3">and TLB misses, as it bounds the number of output destina-</text>
<text top="211" left="81" width="359" height="13" font="3">tions in each pass. This particular implementation has the</text>
<text top="227" left="81" width="359" height="13" font="3">beneﬁt that, by scanning the input twice for each pass, it</text>
<text top="242" left="81" width="359" height="13" font="3">computes exactly how much output space will be required for</text>
<text top="258" left="81" width="359" height="13" font="3">each partition, and hence avoids the synchronization over-</text>
<text top="274" left="81" width="359" height="13" font="3">head that is associated with sharing an output buﬀer. Apart</text>
<text top="290" left="81" width="359" height="13" font="3">from the drawbacks that are associated with any blocking</text>
<text top="305" left="81" width="359" height="13" font="3">algorithm when compared to a non-blocking counterpart,</text>
<text top="321" left="81" width="359" height="13" font="3">this implementation also places a burden on the previous</text>
<text top="337" left="81" width="359" height="13" font="3">operator in a query tree to produce the compact and con-</text>
<text top="352" left="81" width="359" height="13" font="3">tiguous output format that the radix partitioning requires</text>
<text top="368" left="81" width="359" height="13" font="3">as input. Eﬃciently producing a single shared output buﬀer</text>
<text top="384" left="81" width="274" height="13" font="3">is a problem that has been studied before [5].</text>
<text top="413" left="81" width="22" height="16" font="1">3.2</text>
<text top="413" left="121" width="90" height="16" font="1">Build phase</text>
<text top="435" left="94" width="345" height="13" font="3">The build phase proceeds as follows: If the partition phase</text>
<text top="451" left="81" width="359" height="13" font="3">was omitted, then all the threads are assigned to work on</text>
<text top="467" left="81" width="359" height="13" font="3">the relation R. If partitioning was done, then each thread</text>
<text top="483" left="81" width="5" height="12" font="3">i</text>
<text top="482" left="91" width="217" height="13" font="3">is assigned to work on partitions R</text>
<text top="488" left="309" width="4" height="8" font="5">i</text>
<text top="487" left="312" width="27" height="10" font="5">+0∗n</text>
<text top="483" left="340" width="17" height="12" font="3">, R</text>
<text top="488" left="356" width="4" height="8" font="5">i</text>
<text top="487" left="360" width="27" height="10" font="5">+1∗n</text>
<text top="483" left="388" width="17" height="12" font="3">, R</text>
<text top="488" left="404" width="4" height="8" font="5">i</text>
<text top="487" left="408" width="27" height="10" font="5">+2∗n</text>
<text top="482" left="436" width="4" height="13" font="3">,</text>
<text top="498" left="81" width="359" height="13" font="3">etc. For example, a machine with four cores has n = 4, and</text>
<text top="514" left="81" width="228" height="13" font="3">thread 0 would work on partitions R</text>
<text top="519" left="309" width="5" height="9" font="5">0</text>
<text top="514" left="315" width="17" height="12" font="3">, R</text>
<text top="519" left="331" width="5" height="9" font="5">4</text>
<text top="514" left="337" width="17" height="12" font="3">, R</text>
<text top="519" left="354" width="5" height="9" font="5">8</text>
<text top="514" left="360" width="18" height="12" font="3">, ...</text>
<text top="514" left="378" width="62" height="13" font="3">, thread 1</text>
<text top="529" left="81" width="30" height="13" font="3">on R</text>
<text top="534" left="110" width="5" height="9" font="5">1</text>
<text top="530" left="116" width="17" height="12" font="3">, R</text>
<text top="534" left="133" width="5" height="9" font="5">5</text>
<text top="530" left="139" width="17" height="12" font="3">, R</text>
<text top="534" left="156" width="5" height="9" font="5">9</text>
<text top="530" left="162" width="18" height="12" font="3">, ...</text>
<text top="529" left="180" width="30" height="13" font="3">, etc.</text>
<text top="545" left="94" width="345" height="13" font="3">Next, an empty hash table is constructed for each parti-</text>
<text top="561" left="81" width="359" height="13" font="3">tion of the input relation R. To reduce the number of cache</text>
<text top="577" left="81" width="359" height="13" font="3">misses that are incurred during the next (probe) phase, each</text>
<text top="592" left="81" width="359" height="13" font="3">bucket of this hash table is sized so that it ﬁts on a few cache</text>
<text top="608" left="81" width="359" height="13" font="3">lines. Each thread scans every tuple t in its partition, ex-</text>
<text top="624" left="81" width="359" height="13" font="3">tracts the join key k, and then hashes this key using a hash</text>
<text top="639" left="81" width="359" height="13" font="3">function h(·). Then, the tuple t is appended to the end of</text>
<text top="655" left="81" width="359" height="13" font="3">the hash bucket h(k), creating a new hash bucket if neces-</text>
<text top="671" left="81" width="359" height="13" font="3">sary. If the partition phase was omitted, then all the threads</text>
<text top="686" left="81" width="359" height="13" font="3">share the hash table, and writes to each hash bucket have</text>
<text top="702" left="81" width="359" height="13" font="3">to be protected by a latch. The build phase is over when all</text>
<text top="718" left="81" width="340" height="13" font="3">the n threads have processed all the assigned partitions.</text>
<text top="746" left="81" width="22" height="16" font="1">3.3</text>
<text top="746" left="121" width="94" height="16" font="1">Probe phase</text>
<text top="768" left="94" width="345" height="13" font="3">The probe phase schedules work to the n threads in a</text>
<text top="783" left="81" width="359" height="13" font="3">manner similar to the scheduling during the build phase,</text>
<text top="799" left="81" width="359" height="13" font="3">described above. Namely, if no partitioning has been done,</text>
<text top="815" left="81" width="359" height="13" font="3">then all the threads are assigned to S, and they synchronize</text>
<text top="831" left="81" width="359" height="13" font="3">before accessing the read cursor for S. Otherwise, the thread</text>
<text top="847" left="81" width="5" height="12" font="3">i</text>
<text top="846" left="90" width="157" height="13" font="3">is assigned to partitions S</text>
<text top="851" left="247" width="4" height="8" font="5">i</text>
<text top="851" left="251" width="27" height="10" font="5">+0∗n</text>
<text top="847" left="278" width="17" height="12" font="3">, S</text>
<text top="851" left="295" width="4" height="8" font="5">i</text>
<text top="851" left="299" width="27" height="10" font="5">+1∗n</text>
<text top="847" left="326" width="17" height="12" font="3">, S</text>
<text top="851" left="343" width="4" height="8" font="5">i</text>
<text top="851" left="347" width="27" height="10" font="5">+2∗n</text>
<text top="846" left="374" width="30" height="13" font="3">, etc.</text>
<text top="862" left="94" width="345" height="13" font="3">During the probe phase, each thread reads every tuple s</text>
<text top="878" left="81" width="359" height="13" font="3">from its assigned partition and extracts the key k. It then</text>
<text top="893" left="81" width="359" height="13" font="3">checks if the key of each tuple r stored in hash bucket h(k)</text>
<text top="909" left="81" width="359" height="13" font="3">matches k. This check is necessary to ﬁlter out possible</text>
<text top="925" left="81" width="359" height="13" font="3">hash collisions. If the keys match, then the tuples r and s</text>
<text top="940" left="81" width="359" height="13" font="3">are joined to form the output tuple. If the output is mate-</text>
<text top="956" left="81" width="359" height="13" font="3">rialized, it is written to an output buﬀer that is private to</text>
<text top="972" left="81" width="67" height="13" font="3">the thread.</text>
<text top="987" left="94" width="349" height="13" font="3">Notice that there is parallelism even inside the probe phase:</text>
<text top="1003" left="81" width="359" height="13" font="3">looking up the key for each tuple r in a hash bucket and com-</text>
<text top="1019" left="81" width="359" height="13" font="3">paring it to k can be parallelized with the construction of</text>
<text top="1034" left="81" width="359" height="13" font="3">the output tuple, which primarily involves shuﬄing bytes</text>
<text top="1050" left="81" width="359" height="13" font="3">from tuples r and s. (See Section 4.10 for an experiment</text>
<text top="1066" left="81" width="161" height="13" font="3">that explores this further.)</text>
<text top="83" left="475" width="22" height="16" font="1">3.4</text>
<text top="83" left="516" width="147" height="16" font="1">Hash Join Variants</text>
<text top="105" left="489" width="345" height="13" font="3">The algorithms presented above outline an interesting de-</text>
<text top="121" left="475" width="359" height="13" font="3">sign space for hash join algorithms. In this paper, we focus</text>
<text top="136" left="475" width="252" height="13" font="3">on the following four hash join variations:</text>
<text top="163" left="491" width="344" height="13" font="3">1. No partitioning join: An implementation where par-</text>
<text top="179" left="509" width="325" height="13" font="3">titioning is omitted. This implementation creates a</text>
<text top="195" left="509" width="223" height="13" font="3">shared hash table in the build phase.</text>
<text top="216" left="491" width="343" height="13" font="3">2. Shared partitioning join: The ﬁrst non-blocking</text>
<text top="232" left="509" width="325" height="13" font="3">partitioning algorithm of Section 3.1.1, where all the</text>
<text top="247" left="509" width="325" height="13" font="3">threads partition both input sources into shared par-</text>
<text top="263" left="509" width="325" height="13" font="3">titions. Synchronization through a latch is necessary</text>
<text top="279" left="509" width="236" height="13" font="3">before writing to the shared partitions.</text>
<text top="300" left="491" width="343" height="13" font="3">3. Independent partitioning join: The second non-</text>
<text top="316" left="509" width="325" height="13" font="3">blocking partitioning algorithm of Section 3.1.1, where</text>
<text top="332" left="509" width="325" height="13" font="3">all the threads partition both sources and create pri-</text>
<text top="347" left="509" width="92" height="13" font="3">vate partitions.</text>
<text top="369" left="491" width="342" height="13" font="3">4. Radix partitioning join: An implementation where</text>
<text top="384" left="509" width="325" height="13" font="3">each input relation is stored in a single, contiguous</text>
<text top="400" left="509" width="325" height="13" font="3">memory region. Then, each thread participates in the</text>
<text top="416" left="509" width="290" height="13" font="3">radix partitioning, as described in Section 3.1.2.</text>
<text top="460" left="475" width="13" height="16" font="1">4.</text>
<text top="460" left="507" width="266" height="16" font="1">EXPERIMENTAL EVALUATION</text>
<text top="481" left="489" width="345" height="13" font="3">We have implemented the hash join algorithms described</text>
<text top="497" left="475" width="359" height="13" font="3">in Section 3.4 in a stand-alone C++ program. The program</text>
<text top="513" left="475" width="359" height="13" font="3">ﬁrst loads data from the disk into main memory. Data is or-</text>
<text top="528" left="475" width="358" height="13" font="3">ganized in memory using traditional slotted pages. The join</text>
<text top="544" left="475" width="359" height="13" font="3">algorithms are run after the data is loaded in memory. Since</text>
<text top="560" left="475" width="359" height="13" font="3">the focus of this work in on memory-resident datasets, we</text>
<text top="575" left="475" width="359" height="13" font="3">do not consider the time to load the data into main memory</text>
<text top="591" left="475" width="233" height="13" font="3">and only report join completion times.</text>
<text top="607" left="489" width="345" height="13" font="3">For our workload, we wanted to simulate common and</text>
<text top="622" left="475" width="359" height="13" font="3">expensive join operations in decision support environments.</text>
<text top="638" left="475" width="358" height="13" font="3">The execution of a decision support query in a data ware-</text>
<text top="654" left="475" width="359" height="13" font="3">house typically involves multiple phases. First, one or more</text>
<text top="670" left="475" width="359" height="13" font="3">dimension relations are reduced based on the selection con-</text>
<text top="685" left="475" width="359" height="13" font="3">straints. Then, these dimension relations are combined into</text>
<text top="701" left="475" width="358" height="13" font="3">an intermediate one, which is then joined with a much larger</text>
<text top="717" left="475" width="359" height="13" font="3">fact relation. Finally, aggregate statistics on the join output</text>
<text top="732" left="475" width="359" height="13" font="3">are computed and returned to the user. For example, in the</text>
<text top="748" left="475" width="359" height="13" font="3">TPC-H decision support benchmark, this execution pattern</text>
<text top="764" left="475" width="278" height="13" font="3">is encountered in at least 15 of the 22 queries.</text>
<text top="779" left="489" width="345" height="13" font="3">We try to capture the essence of this operation by focusing</text>
<text top="795" left="475" width="358" height="13" font="3">on the most expensive component, namely the join operation</text>
<text top="811" left="475" width="359" height="13" font="3">between the intermediate relation R (the outcome of various</text>
<text top="826" left="475" width="358" height="13" font="3">operations on the dimension relations) with a much larger</text>
<text top="842" left="475" width="358" height="13" font="3">fact relation S. To allow us to focus on the core join perfor-</text>
<text top="858" left="475" width="358" height="13" font="3">mance, we initially do not consider the cost of materializing</text>
<text top="897" left="668" width="91" height="9" font="4">Intel Nehalem</text>
<text top="911" left="555" width="27" height="12" font="4">CPU</text>
<text top="911" left="646" width="134" height="12" font="4">Xeon X5650 @ 2.67GHz</text>
<text top="925" left="553" width="31" height="12" font="4">Cores</text>
<text top="925" left="710" width="6" height="12" font="4">6</text>
<text top="939" left="520" width="98" height="12" font="4">Contexts per core</text>
<text top="939" left="710" width="6" height="12" font="4">2</text>
<text top="953" left="516" width="106" height="12" font="4">Cache size, sharing</text>
<text top="953" left="665" width="96" height="12" font="4">12MB L3, shared</text>
<text top="968" left="546" width="46" height="12" font="4">Memory</text>
<text top="968" left="672" width="82" height="12" font="4">3x 4GB DDR3</text>
<text top="985" left="646" width="135" height="9" font="4">Sun UltraSPARC T2</text>
<text top="999" left="555" width="27" height="12" font="4">CPU</text>
<text top="999" left="638" width="151" height="12" font="4">UltraSPARC T2 @ 1.2GHz</text>
<text top="1013" left="553" width="31" height="12" font="4">Cores</text>
<text top="1013" left="710" width="6" height="12" font="4">8</text>
<text top="1027" left="520" width="98" height="12" font="4">Contexts per core</text>
<text top="1027" left="710" width="6" height="12" font="4">8</text>
<text top="1041" left="516" width="106" height="12" font="4">Cache size, sharing</text>
<text top="1041" left="669" width="89" height="12" font="4">4MB L2, shared</text>
<text top="1056" left="546" width="46" height="12" font="4">Memory</text>
<text top="1056" left="672" width="82" height="12" font="4">8x 2GB DDR2</text>
<text top="1080" left="538" width="233" height="13" font="3">Table 1: Platform characteristics.</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="8" size="6" family="Times" color="#000000"/>
	<fontspec id="9" size="8" family="Times" color="#000000"/>
<text top="263" left="142" width="5" height="8" font="8">0</text>
<text top="232" left="133" width="14" height="8" font="8">100</text>
<text top="202" left="133" width="14" height="8" font="8">200</text>
<text top="172" left="133" width="14" height="8" font="8">300</text>
<text top="142" left="133" width="14" height="8" font="8">400</text>
<text top="111" left="133" width="14" height="8" font="8">500</text>
<text top="81" left="133" width="14" height="8" font="8">600</text>
<text top="269" left="167" width="0" height="8" font="8">1</text>
<text top="274" left="194" width="7" height="8" font="8">16 64</text>
<text top="278" left="207" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="253" width="0" height="8" font="8">12</text>
<text top="275" left="253" width="0" height="8" font="8">8K</text>
<text top="274" left="280" width="7" height="8" font="8">16 64</text>
<text top="278" left="293" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="340" width="0" height="8" font="8">12</text>
<text top="275" left="340" width="0" height="8" font="8">8K</text>
<text top="274" left="366" width="7" height="8" font="8">16 64</text>
<text top="278" left="380" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="426" width="0" height="8" font="8">12</text>
<text top="275" left="426" width="0" height="8" font="8">8K</text>
<text top="211" left="122" width="0" height="8" font="8">Cycl</text>
<text top="195" left="122" width="0" height="8" font="8">es pe</text>
<text top="175" left="122" width="0" height="8" font="8">r </text>
<text top="170" left="122" width="0" height="8" font="8">o</text>
<text top="166" left="122" width="0" height="8" font="8">utpu</text>
<text top="150" left="122" width="0" height="8" font="8">t t</text>
<text top="143" left="122" width="0" height="8" font="8">u</text>
<text top="139" left="122" width="0" height="8" font="8">pl</text>
<text top="132" left="122" width="0" height="8" font="8">e</text>
<text top="324" left="254" width="73" height="8" font="8">Number of partitions</text>
<text top="307" left="368" width="39" height="8" font="8">Radix-best</text>
<text top="307" left="278" width="45" height="8" font="8">Independent</text>
<text top="307" left="201" width="26" height="8" font="8">Shared</text>
<text top="307" left="156" width="10" height="8" font="8">No</text>
<text top="86" left="220" width="38" height="10" font="9">partition</text>
<text top="87" left="305" width="23" height="10" font="9">build</text>
<text top="86" left="374" width="28" height="10" font="9">probe</text>
<text top="338" left="220" width="107" height="13" font="3">(a) Intel Nehalem</text>
<text top="263" left="505" width="5" height="8" font="8">0</text>
<text top="242" left="500" width="9" height="8" font="8">20</text>
<text top="222" left="500" width="9" height="8" font="8">40</text>
<text top="202" left="500" width="9" height="8" font="8">60</text>
<text top="182" left="500" width="9" height="8" font="8">80</text>
<text top="162" left="496" width="14" height="8" font="8">100</text>
<text top="142" left="496" width="14" height="8" font="8">120</text>
<text top="121" left="496" width="14" height="8" font="8">140</text>
<text top="101" left="496" width="14" height="8" font="8">160</text>
<text top="81" left="496" width="14" height="8" font="8">180</text>
<text top="269" left="531" width="0" height="8" font="8">1</text>
<text top="274" left="568" width="0" height="8" font="8">64</text>
<text top="278" left="575" width="44" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="626" width="0" height="8" font="8">12</text>
<text top="275" left="626" width="0" height="8" font="8">8K</text>
<text top="274" left="663" width="0" height="8" font="8">64</text>
<text top="278" left="671" width="22" height="8" font="8">256 512 1K 2K</text>
<text top="274" left="729" width="0" height="8" font="8">64</text>
<text top="278" left="737" width="44" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="788" width="0" height="8" font="8">12</text>
<text top="275" left="788" width="0" height="8" font="8">8K</text>
<text top="211" left="485" width="0" height="8" font="8">Cycl</text>
<text top="195" left="485" width="0" height="8" font="8">es pe</text>
<text top="175" left="485" width="0" height="8" font="8">r </text>
<text top="170" left="485" width="0" height="8" font="8">o</text>
<text top="166" left="485" width="0" height="8" font="8">utpu</text>
<text top="150" left="485" width="0" height="8" font="8">t t</text>
<text top="143" left="485" width="0" height="8" font="8">u</text>
<text top="139" left="485" width="0" height="8" font="8">pl</text>
<text top="132" left="485" width="0" height="8" font="8">e</text>
<text top="324" left="617" width="73" height="8" font="8">Number of partitions</text>
<text top="307" left="726" width="39" height="8" font="8">Radix-best</text>
<text top="307" left="642" width="45" height="8" font="8">Independent</text>
<text top="307" left="571" width="26" height="8" font="8">Shared</text>
<text top="307" left="520" width="10" height="8" font="8">No</text>
<text top="86" left="582" width="38" height="10" font="9">partition</text>
<text top="87" left="668" width="23" height="10" font="9">build</text>
<text top="86" left="737" width="28" height="10" font="9">probe</text>
<text top="338" left="561" width="150" height="13" font="3">(b) Sun UltraSPARC T2</text>
<text top="361" left="253" width="408" height="13" font="3">Figure 1: Cycles per output tuple for the uniform dataset.</text>
<text top="391" left="81" width="359" height="13" font="3">the output in memory, adopting a similar method as pre-</text>
<text top="407" left="81" width="359" height="13" font="3">vious work [7, 11]. In later experiments (see Section 4.8),</text>
<text top="423" left="81" width="359" height="13" font="3">we consider the eﬀect of materializing the join result – in</text>
<text top="438" left="81" width="359" height="13" font="3">these cases, the join result is created in main memory and</text>
<text top="454" left="81" width="117" height="13" font="3">not ﬂushed to disk.</text>
<text top="470" left="94" width="345" height="13" font="3">We describe the synthetic datasets that we used in the</text>
<text top="485" left="81" width="359" height="13" font="3">next section (Section 4.1). In Section 4.2 we give details</text>
<text top="501" left="81" width="359" height="13" font="3">about the hardware that we used for our experiments. We</text>
<text top="517" left="81" width="359" height="13" font="3">continue with a presentation of the results in Sections 4.3</text>
<text top="533" left="81" width="51" height="13" font="3">and 4.4.</text>
<text top="533" left="146" width="293" height="13" font="3">We analyze the results further in Sections 4.5</text>
<text top="548" left="81" width="359" height="13" font="3">through 4.7. We present results investigating the eﬀect of</text>
<text top="564" left="81" width="359" height="13" font="3">output materialization, and the sensitivity to input sizes and</text>
<text top="580" left="81" width="246" height="13" font="3">selectivities in Sections 4.8 through 4.10.</text>
<text top="608" left="81" width="22" height="16" font="1">4.1</text>
<text top="608" left="121" width="58" height="16" font="1">Dataset</text>
<text top="629" left="94" width="345" height="13" font="3">We experimented with three diﬀerent datasets, which we</text>
<text top="645" left="81" width="359" height="13" font="3">denote as uniform, low skew and high skew, respectively. We</text>
<text top="661" left="81" width="359" height="13" font="3">assume that the relation R contains the primary key and the</text>
<text top="676" left="81" width="359" height="13" font="3">relation S contains a foreign key referencing tuples in R. In</text>
<text top="692" left="81" width="359" height="13" font="3">all the datasets we ﬁx the cardinalities of R to 16M tuples</text>
<text top="708" left="81" width="135" height="13" font="3">and S to 256M tuples</text>
<text top="705" left="216" width="5" height="9" font="5">1</text>
<text top="708" left="222" width="217" height="13" font="3">. We picked the ratio of R to S to</text>
<text top="723" left="81" width="359" height="13" font="3">be 1:16 to mimic the common decision support settings. We</text>
<text top="739" left="81" width="282" height="13" font="3">experiment with diﬀerent ratios in Section 4.9.</text>
<text top="755" left="94" width="345" height="13" font="3">In our experiments both keys and payloads are eight bytes</text>
<text top="771" left="81" width="359" height="13" font="3">each. Each tuple is simply a &lt;key, payload&gt; pair, so tuples</text>
<text top="786" left="81" width="359" height="13" font="3">are 16 bytes long. Keys can either be the values themselves,</text>
<text top="802" left="81" width="359" height="13" font="3">if the key is numeric, or an 8-byte hash of the value in the</text>
<text top="818" left="81" width="359" height="13" font="3">case of strings. We chose to represent payloads as 8 bytes for</text>
<text top="833" left="81" width="359" height="13" font="3">two reasons: (a) Given that columnar storage is commonly</text>
<text top="849" left="81" width="359" height="13" font="3">used in data warehouses, we want to simulate storing &lt;key,</text>
<text top="865" left="81" width="359" height="13" font="3">value&gt; or &lt;key, record-id&gt; pairs in the hash table, and (b)</text>
<text top="880" left="81" width="358" height="13" font="3">make comparisons with existing work (i.e. [11, 4]) easier.</text>
<text top="896" left="81" width="359" height="13" font="3">Exploring alternative ways of constructing hash table entries</text>
<text top="912" left="81" width="358" height="13" font="3">is not a focus of this work, but has been explored before [15].</text>
<text top="927" left="94" width="345" height="13" font="3">For the uniform dataset, we create tuples in the relation</text>
<text top="944" left="81" width="8" height="12" font="3">S</text>
<text top="943" left="95" width="345" height="13" font="3">such that each tuple matches every key in the relation R</text>
<text top="959" left="81" width="359" height="13" font="3">with equal probability. For the skewed datasets, we added</text>
<text top="975" left="81" width="359" height="13" font="3">skew to the distribution of the foreign keys in the relation S.</text>
<text top="990" left="81" width="359" height="13" font="3">(Adding skew to the relation R would violate the primary</text>
<text top="1006" left="81" width="359" height="13" font="3">key constraint.) We created two skewed datasets, for two</text>
<text top="1021" left="81" width="359" height="13" font="3">diﬀerent s values of the Zipf distribution: low skew with</text>
<text top="1038" left="81" width="6" height="12" font="3">s</text>
<text top="1037" left="91" width="348" height="13" font="3">= 1.05 and high skew with s = 1.25. Intuitively, the most</text>
<text top="1064" left="81" width="5" height="9" font="5">1</text>
<text top="1066" left="88" width="173" height="13" font="3">Throughout the paper, M=2</text>
<text top="1064" left="261" width="11" height="9" font="5">20</text>
<text top="1066" left="277" width="55" height="13" font="3">and K=2</text>
<text top="1064" left="333" width="11" height="9" font="5">10</text>
<text top="1066" left="344" width="4" height="13" font="3">.</text>
<text top="391" left="475" width="359" height="13" font="3">popular key appears in the low skew dataset 8% of the time,</text>
<text top="407" left="475" width="359" height="13" font="3">and the ten most popular keys account for 24% of the keys.</text>
<text top="423" left="475" width="359" height="13" font="3">In comparison, in the high skew dataset, the most popular</text>
<text top="438" left="475" width="359" height="13" font="3">key appears 22% of the time, and the ten most popular keys</text>
<text top="454" left="475" width="146" height="13" font="3">appear 52% of the time.</text>
<text top="470" left="489" width="345" height="13" font="3">In all the experiments, the hash buckets that are created</text>
<text top="485" left="475" width="359" height="13" font="3">during the build phase have a ﬁxed size: they always have</text>
<text top="501" left="475" width="359" height="13" font="3">32 bytes of space for the payload, and 8 bytes are reserved</text>
<text top="517" left="475" width="359" height="13" font="3">for the pointer that points to the next hash bucket in case of</text>
<text top="533" left="475" width="359" height="13" font="3">overﬂow. These numbers were picked so that each bucket ﬁts</text>
<text top="548" left="475" width="359" height="13" font="3">in a single, last-level cache line for both the architectures.</text>
<text top="564" left="475" width="359" height="13" font="3">We size the hash table appropriately so that no overﬂow</text>
<text top="580" left="475" width="42" height="13" font="3">occurs.</text>
<text top="620" left="475" width="22" height="16" font="1">4.2</text>
<text top="620" left="516" width="75" height="16" font="1">Platforms</text>
<text top="642" left="489" width="345" height="13" font="3">We evaluated our methods on two diﬀerent architectures:</text>
<text top="658" left="475" width="359" height="13" font="3">the Intel Nehalem and the Sun UltraSPARC T2. We de-</text>
<text top="674" left="475" width="359" height="13" font="3">scribe the characteristics of each architecture in detail below,</text>
<text top="689" left="475" width="278" height="13" font="3">and we summarize key parameters in Table 1.</text>
<text top="705" left="489" width="345" height="13" font="3">The Intel Nehalem microarchitecture is the successor of</text>
<text top="721" left="475" width="359" height="13" font="3">the Intel Core microarchitecture. All Nehalem-based CPUs</text>
<text top="736" left="475" width="359" height="13" font="3">are superscalar processors and exploit instruction-level par-</text>
<text top="752" left="475" width="358" height="13" font="3">allelism by using out-of-order execution. The Nehalem fam-</text>
<text top="768" left="475" width="359" height="13" font="3">ily supports multi-threading, and allows two contexts to ex-</text>
<text top="783" left="475" width="89" height="13" font="3">ecute per core.</text>
<text top="799" left="489" width="345" height="13" font="3">For our experiments, we use the six-core Intel Xeon X5650</text>
<text top="815" left="475" width="359" height="13" font="3">that was released in Q1 of 2010. This CPU has a uniﬁed</text>
<text top="831" left="475" width="359" height="13" font="3">12MB, 16-way associative L3 cache with a line size of 64</text>
<text top="846" left="475" width="359" height="13" font="3">bytes. This L3 cache is shared by all twelve contexts ex-</text>
<text top="862" left="475" width="358" height="13" font="3">ecuting on the six cores. Each core has a private 256KB,</text>
<text top="878" left="475" width="359" height="13" font="3">8-way associative L2 cache, with a line size of 64 bytes. Fi-</text>
<text top="893" left="475" width="359" height="13" font="3">nally, private 32KB instruction and data L1 caches connect</text>
<text top="909" left="475" width="186" height="13" font="3">to each core’s load/store units.</text>
<text top="925" left="489" width="345" height="13" font="3">The Sun UltraSPARC T2 was introduced in 2007 and re-</text>
<text top="940" left="475" width="360" height="13" font="3">lies heavily on multi-threading to achieve maximum through-</text>
<text top="956" left="475" width="359" height="13" font="3">put. An UltraSPARC T2 chip has eight cores and each core</text>
<text top="972" left="475" width="359" height="13" font="3">has hardware support for eight contexts. UltraSPARC T2</text>
<text top="987" left="475" width="359" height="13" font="3">does not feature out-of-order execution. Each core has a</text>
<text top="1003" left="475" width="359" height="13" font="3">single instruction fetch unit, a single ﬂoating point unit, a</text>
<text top="1019" left="475" width="359" height="13" font="3">single memory unit and two arithmetic units. At every cy-</text>
<text top="1034" left="475" width="359" height="13" font="3">cle, each core executes at most two instructions, each taken</text>
<text top="1050" left="475" width="359" height="13" font="3">from two diﬀerent contexts. Each context is scheduled in a</text>
<text top="1066" left="475" width="359" height="13" font="3">round-robin fashion every cycle, unless the context has ini-</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="1188" width="918">
<text top="263" left="142" width="5" height="8" font="8">0</text>
<text top="232" left="133" width="14" height="8" font="8">100</text>
<text top="202" left="133" width="14" height="8" font="8">200</text>
<text top="172" left="133" width="14" height="8" font="8">300</text>
<text top="142" left="133" width="14" height="8" font="8">400</text>
<text top="111" left="133" width="14" height="8" font="8">500</text>
<text top="81" left="133" width="14" height="8" font="8">600</text>
<text top="269" left="167" width="0" height="8" font="8">1</text>
<text top="274" left="194" width="7" height="8" font="8">16 64</text>
<text top="278" left="207" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="253" width="0" height="8" font="8">12</text>
<text top="275" left="253" width="0" height="8" font="8">8K</text>
<text top="274" left="280" width="7" height="8" font="8">16 64</text>
<text top="278" left="293" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="340" width="0" height="8" font="8">12</text>
<text top="275" left="340" width="0" height="8" font="8">8K</text>
<text top="274" left="366" width="7" height="8" font="8">16 64</text>
<text top="278" left="380" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="426" width="0" height="8" font="8">12</text>
<text top="275" left="426" width="0" height="8" font="8">8K</text>
<text top="211" left="122" width="0" height="8" font="8">Cycl</text>
<text top="195" left="122" width="0" height="8" font="8">es pe</text>
<text top="175" left="122" width="0" height="8" font="8">r </text>
<text top="170" left="122" width="0" height="8" font="8">o</text>
<text top="166" left="122" width="0" height="8" font="8">utpu</text>
<text top="150" left="122" width="0" height="8" font="8">t t</text>
<text top="143" left="122" width="0" height="8" font="8">u</text>
<text top="139" left="122" width="0" height="8" font="8">pl</text>
<text top="132" left="122" width="0" height="8" font="8">e</text>
<text top="324" left="254" width="73" height="8" font="8">Number of partitions</text>
<text top="307" left="368" width="39" height="8" font="8">Radix-best</text>
<text top="307" left="278" width="45" height="8" font="8">Independent</text>
<text top="307" left="201" width="26" height="8" font="8">Shared</text>
<text top="307" left="156" width="10" height="8" font="8">No</text>
<text top="86" left="220" width="38" height="10" font="9">partition</text>
<text top="87" left="305" width="23" height="10" font="9">build</text>
<text top="86" left="374" width="28" height="10" font="9">probe</text>
<text top="338" left="220" width="107" height="13" font="3">(a) Intel Nehalem</text>
<text top="263" left="505" width="5" height="8" font="8">0</text>
<text top="244" left="500" width="9" height="8" font="8">50</text>
<text top="226" left="496" width="14" height="8" font="8">100</text>
<text top="208" left="496" width="14" height="8" font="8">150</text>
<text top="190" left="496" width="14" height="8" font="8">200</text>
<text top="172" left="496" width="14" height="8" font="8">250</text>
<text top="154" left="496" width="14" height="8" font="8">300</text>
<text top="135" left="496" width="14" height="8" font="8">350</text>
<text top="117" left="496" width="14" height="8" font="8">400</text>
<text top="99" left="496" width="14" height="8" font="8">450</text>
<text top="81" left="496" width="14" height="8" font="8">500</text>
<text top="269" left="532" width="0" height="8" font="8">1</text>
<text top="274" left="564" width="0" height="8" font="8">64</text>
<text top="278" left="572" width="0" height="8" font="8">256</text>
<text top="278" left="580" width="0" height="8" font="8">512</text>
<text top="275" left="588" width="0" height="8" font="8">1K</text>
<text top="275" left="596" width="0" height="8" font="8">2K</text>
<text top="275" left="604" width="0" height="8" font="8">4K</text>
<text top="275" left="612" width="0" height="8" font="8">8K</text>
<text top="279" left="620" width="0" height="8" font="8">32K</text>
<text top="284" left="628" width="0" height="8" font="8">12</text>
<text top="275" left="628" width="0" height="8" font="8">8K</text>
<text top="274" left="660" width="0" height="8" font="8">64</text>
<text top="278" left="668" width="0" height="8" font="8">256</text>
<text top="278" left="676" width="0" height="8" font="8">512</text>
<text top="275" left="684" width="0" height="8" font="8">1K</text>
<text top="275" left="692" width="0" height="8" font="8">2K</text>
<text top="274" left="724" width="0" height="8" font="8">64</text>
<text top="278" left="732" width="0" height="8" font="8">256</text>
<text top="278" left="739" width="0" height="8" font="8">512</text>
<text top="275" left="747" width="0" height="8" font="8">1K</text>
<text top="275" left="755" width="0" height="8" font="8">2K</text>
<text top="275" left="763" width="0" height="8" font="8">4K</text>
<text top="275" left="771" width="0" height="8" font="8">8K</text>
<text top="279" left="779" width="0" height="8" font="8">32K</text>
<text top="284" left="787" width="0" height="8" font="8">12</text>
<text top="275" left="787" width="0" height="8" font="8">8K</text>
<text top="211" left="485" width="0" height="8" font="8">Cycl</text>
<text top="195" left="485" width="0" height="8" font="8">es pe</text>
<text top="175" left="485" width="0" height="8" font="8">r </text>
<text top="170" left="485" width="0" height="8" font="8">o</text>
<text top="166" left="485" width="0" height="8" font="8">utpu</text>
<text top="150" left="485" width="0" height="8" font="8">t t</text>
<text top="143" left="485" width="0" height="8" font="8">u</text>
<text top="139" left="485" width="0" height="8" font="8">pl</text>
<text top="132" left="485" width="0" height="8" font="8">e</text>
<text top="324" left="617" width="73" height="8" font="8">Number of partitions</text>
<text top="307" left="726" width="39" height="8" font="8">Radix-best</text>
<text top="307" left="643" width="45" height="8" font="8">Independent</text>
<text top="307" left="573" width="26" height="8" font="8">Shared</text>
<text top="307" left="521" width="10" height="8" font="8">No</text>
<text top="86" left="582" width="38" height="10" font="9">partition</text>
<text top="87" left="668" width="23" height="10" font="9">build</text>
<text top="86" left="737" width="28" height="10" font="9">probe</text>
<text top="338" left="561" width="150" height="13" font="3">(b) Sun UltraSPARC T2</text>
<text top="361" left="250" width="415" height="13" font="3">Figure 2: Cycles per output tuple for the low skew dataset.</text>
<text top="391" left="81" width="359" height="13" font="3">tiated a long-latency operation, such as a memory load that</text>
<text top="407" left="81" width="326" height="13" font="3">caused a cache miss, and has to wait for the outcome.</text>
<text top="423" left="94" width="345" height="13" font="3">At the bottom of the cache hierarchy of the UltraSPARC</text>
<text top="438" left="81" width="359" height="13" font="3">T2 chip lies a shared 4MB, 16-way associative write-back L2</text>
<text top="454" left="81" width="359" height="13" font="3">cache, with a line size of 64 bytes. To maximize through-</text>
<text top="470" left="81" width="359" height="13" font="3">put, the shared cache is physically split into eight banks.</text>
<text top="485" left="81" width="359" height="13" font="3">Therefore, up to eight cache requests can be handled concur-</text>
<text top="501" left="81" width="359" height="13" font="3">rently, provided that each request hits a diﬀerent bank. Each</text>
<text top="517" left="81" width="359" height="13" font="3">core connects to this shared cache through a non-blocking,</text>
<text top="533" left="81" width="359" height="13" font="3">pipelined crossbar. Finally, each core has a 8KB, 4-way</text>
<text top="548" left="81" width="359" height="13" font="3">associative write-through L1 data cache with 16 bytes per</text>
<text top="564" left="81" width="359" height="13" font="3">cache line that is shared by all the eight hardware contexts.</text>
<text top="580" left="81" width="359" height="13" font="3">Overall, in the absence of arbitration delays, the L2 cache</text>
<text top="595" left="81" width="140" height="13" font="3">hit latency is 20 cycles.</text>
<text top="636" left="81" width="22" height="16" font="1">4.3</text>
<text top="636" left="121" width="56" height="16" font="1">Results</text>
<text top="658" left="94" width="345" height="13" font="3">We start with the uniform dataset. In Figure 1, we plot</text>
<text top="674" left="81" width="359" height="13" font="3">the average number of CPU cycles that it takes to produce</text>
<text top="689" left="81" width="359" height="13" font="3">one output tuple, without actually writing the output, for</text>
<text top="705" left="81" width="359" height="13" font="3">a varying number of partitions. (Note that to convert the</text>
<text top="721" left="81" width="359" height="13" font="3">CPU cycles to wall clock time, we simply divide the CPU</text>
<text top="736" left="81" width="359" height="13" font="3">cycles by the corresponding clock rate shown in Table 1).</text>
<text top="752" left="81" width="359" height="13" font="3">The horizontal axis shows the diﬀerent join algorithms (bars</text>
<text top="768" left="79" width="360" height="13" font="3">“No”, “Shared”, “Independent”), corresponding to the ﬁrst</text>
<text top="783" left="81" width="359" height="13" font="3">three hash join variants described in Section 3.4. For the</text>
<text top="799" left="81" width="359" height="13" font="3">radix join algorithm, we show the best result across any</text>
<text top="815" left="81" width="359" height="13" font="3">number of passes (bars marked “Radix-best”). Notice that</text>
<text top="831" left="81" width="359" height="13" font="3">we assume that the optimizer will always be correct and pick</text>
<text top="846" left="81" width="181" height="13" font="3">the optimal number of passes.</text>
<text top="862" left="94" width="345" height="13" font="3">Overall, the build phase takes a very small fraction of</text>
<text top="878" left="81" width="359" height="13" font="3">the overall time, regardless of the partitioning strategy that</text>
<text top="893" left="81" width="359" height="13" font="3">is being used, across all architectures (see Figure 1). The</text>
<text top="909" left="81" width="359" height="13" font="3">reason for this behavior is two-fold. First and foremost, the</text>
<text top="925" left="81" width="359" height="13" font="3">smaller cardinality of the R relation translates into less work</text>
<text top="940" left="81" width="359" height="13" font="3">during the build phase. (We experiment with diﬀerent car-</text>
<text top="956" left="81" width="359" height="13" font="3">dinality ratios in Section 4.9.) Second, building a hash table</text>
<text top="972" left="81" width="359" height="13" font="3">is a really simple operation: it merely involves copying the</text>
<text top="987" left="81" width="359" height="13" font="3">input data into the appropriate hash bucket, which incurs a</text>
<text top="1003" left="81" width="359" height="13" font="3">lot less computation than the other steps, such as the out-</text>
<text top="1019" left="81" width="359" height="13" font="3">put tuple reconstruction that must take place in the probe</text>
<text top="1034" left="81" width="359" height="13" font="3">phase. The performance of the join operation is therefore</text>
<text top="1050" left="81" width="359" height="13" font="3">mostly determined by the time spent partitioning the input</text>
<text top="1066" left="81" width="223" height="13" font="3">relations and probing the hash table.</text>
<text top="391" left="489" width="345" height="13" font="3">As can be observed in Figure 1(a) for the Intel Nehalem</text>
<text top="407" left="475" width="358" height="13" font="3">architecture, the performance of the non-partitioned join al-</text>
<text top="423" left="475" width="359" height="13" font="3">gorithm is comparable to the optimal performance achieved</text>
<text top="438" left="475" width="359" height="13" font="3">by the partition-based algorithms. The shared partitioning</text>
<text top="454" left="475" width="359" height="13" font="3">algorithm performs best when sizing partitions so that they</text>
<text top="470" left="475" width="358" height="13" font="3">ﬁt in the last level cache. This ﬁgure reveals a problem with</text>
<text top="485" left="475" width="359" height="13" font="3">the independent partitioning algorithm. For a high number</text>
<text top="501" left="475" width="359" height="13" font="3">of partitions, say 128K, each thread will create its own pri-</text>
<text top="517" left="475" width="359" height="13" font="3">vate buﬀer, for a total of 128K ∗ 12 ≈ 1.5 million output</text>
<text top="533" left="475" width="359" height="13" font="3">buﬀers. This high number of temporary buﬀers introduces</text>
<text top="548" left="475" width="359" height="13" font="3">two problems. First, it results in poor space utilization, as</text>
<text top="564" left="475" width="359" height="13" font="3">most of these buﬀers are ﬁlled with very few tuples. Sec-</text>
<text top="580" left="475" width="359" height="13" font="3">ond, the working set of the algorithm grows tremendously,</text>
<text top="595" left="475" width="359" height="13" font="3">and keeping track of 1.5 million cache lines requires a cache</text>
<text top="611" left="475" width="358" height="13" font="3">whose capacity is orders of magnitude larger than the 12MB</text>
<text top="627" left="475" width="359" height="13" font="3">L3 cache. The radix partitioning algorithm is not aﬀected</text>
<text top="642" left="475" width="359" height="13" font="3">by this problem, because it operates in multiple passes and</text>
<text top="658" left="475" width="352" height="13" font="3">limits the number of partition output buﬀers in each pass.</text>
<text top="674" left="489" width="345" height="13" font="3">Next, we experimented with the Sun UltraSPARC T2 ar-</text>
<text top="690" left="475" width="359" height="13" font="3">chitecture. In Figure 1(b) we see that doing no partitioning</text>
<text top="705" left="475" width="358" height="13" font="3">is at least 1.5X faster compared to all the other algorithms.</text>
<text top="721" left="475" width="359" height="13" font="3">The limited memory on this machine prevented us from run-</text>
<text top="737" left="475" width="359" height="13" font="3">ning experiments with a high number of partitions for the</text>
<text top="752" left="475" width="359" height="13" font="3">independent partitioning algorithm because of the signiﬁ-</text>
<text top="768" left="475" width="359" height="13" font="3">cant memory overhead discussed in the previous paragraph.</text>
<text top="784" left="475" width="359" height="13" font="3">As this machine supports nearly ﬁve times more hardware</text>
<text top="799" left="475" width="359" height="13" font="3">contexts than the Intel machine, the memory that is required</text>
<text top="815" left="475" width="263" height="13" font="3">for bookkeeping is ﬁve times higher as well.</text>
<text top="831" left="489" width="345" height="13" font="3">To summarize our results with the uniform dataset, we</text>
<text top="846" left="475" width="359" height="13" font="3">see that on the Intel architecture the performance of the no</text>
<text top="862" left="475" width="359" height="13" font="3">partitioning join algorithm is comparable to the performance</text>
<text top="878" left="475" width="359" height="13" font="3">of all the other algorithms. For the Sun UltraSPARC T2,</text>
<text top="894" left="475" width="359" height="13" font="3">we see that the no partitioning join algorithm outperforms</text>
<text top="909" left="475" width="359" height="13" font="3">the other algorithms by at least 1.5X. Additionally, the no</text>
<text top="925" left="475" width="359" height="13" font="3">partitioning algorithm is more robust, as the performance</text>
<text top="940" left="475" width="359" height="13" font="3">of the other algorithms degrades if the query optimizer does</text>
<text top="956" left="475" width="337" height="13" font="3">not pick the optimal value for the number of partitions.</text>
<text top="981" left="475" width="22" height="16" font="1">4.4</text>
<text top="981" left="516" width="107" height="16" font="1">Effect of skew</text>
<text top="1003" left="489" width="345" height="13" font="3">We now consider the case when the distribution of foreign</text>
<text top="1019" left="475" width="359" height="13" font="3">keys in the relation S is skewed. We again plot the average</text>
<text top="1034" left="475" width="359" height="13" font="3">time to produce each tuple of the join (in machine cycles)</text>
<text top="1050" left="475" width="359" height="13" font="3">in Figure 2 for the low skew dataset, and in Figure 3 for the</text>
<text top="1066" left="475" width="111" height="13" font="3">high skew dataset.</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="1188" width="918">
<text top="263" left="142" width="5" height="8" font="8">0</text>
<text top="232" left="133" width="14" height="8" font="8">100</text>
<text top="202" left="133" width="14" height="8" font="8">200</text>
<text top="172" left="133" width="14" height="8" font="8">300</text>
<text top="142" left="133" width="14" height="8" font="8">400</text>
<text top="111" left="133" width="14" height="8" font="8">500</text>
<text top="81" left="133" width="14" height="8" font="8">600</text>
<text top="269" left="167" width="0" height="8" font="8">1</text>
<text top="274" left="194" width="7" height="8" font="8">16 64</text>
<text top="278" left="207" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="253" width="0" height="8" font="8">12</text>
<text top="275" left="253" width="0" height="8" font="8">8K</text>
<text top="274" left="280" width="7" height="8" font="8">16 64</text>
<text top="278" left="293" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="340" width="0" height="8" font="8">12</text>
<text top="275" left="340" width="0" height="8" font="8">8K</text>
<text top="274" left="366" width="7" height="8" font="8">16 64</text>
<text top="278" left="380" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="426" width="0" height="8" font="8">12</text>
<text top="275" left="426" width="0" height="8" font="8">8K</text>
<text top="211" left="122" width="0" height="8" font="8">Cycl</text>
<text top="195" left="122" width="0" height="8" font="8">es pe</text>
<text top="175" left="122" width="0" height="8" font="8">r </text>
<text top="170" left="122" width="0" height="8" font="8">o</text>
<text top="166" left="122" width="0" height="8" font="8">utpu</text>
<text top="150" left="122" width="0" height="8" font="8">t t</text>
<text top="143" left="122" width="0" height="8" font="8">u</text>
<text top="139" left="122" width="0" height="8" font="8">pl</text>
<text top="132" left="122" width="0" height="8" font="8">e</text>
<text top="324" left="254" width="73" height="8" font="8">Number of partitions</text>
<text top="307" left="368" width="39" height="8" font="8">Radix-best</text>
<text top="307" left="278" width="45" height="8" font="8">Independent</text>
<text top="307" left="201" width="26" height="8" font="8">Shared</text>
<text top="307" left="156" width="10" height="8" font="8">No</text>
<text top="86" left="220" width="38" height="10" font="9">partition</text>
<text top="87" left="305" width="23" height="10" font="9">build</text>
<text top="86" left="374" width="28" height="10" font="9">probe</text>
<text top="338" left="220" width="107" height="13" font="3">(a) Intel Nehalem</text>
<text top="263" left="505" width="5" height="8" font="8">0</text>
<text top="237" left="496" width="14" height="8" font="8">100</text>
<text top="211" left="496" width="14" height="8" font="8">200</text>
<text top="185" left="496" width="14" height="8" font="8">300</text>
<text top="159" left="496" width="14" height="8" font="8">400</text>
<text top="133" left="496" width="14" height="8" font="8">500</text>
<text top="107" left="496" width="14" height="8" font="8">600</text>
<text top="81" left="496" width="14" height="8" font="8">700</text>
<text top="269" left="531" width="0" height="8" font="8">1</text>
<text top="274" left="568" width="0" height="8" font="8">64</text>
<text top="278" left="575" width="44" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="626" width="0" height="8" font="8">12</text>
<text top="275" left="626" width="0" height="8" font="8">8K</text>
<text top="274" left="663" width="0" height="8" font="8">64</text>
<text top="278" left="671" width="22" height="8" font="8">256 512 1K 2K</text>
<text top="274" left="729" width="0" height="8" font="8">64</text>
<text top="278" left="737" width="44" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="788" width="0" height="8" font="8">12</text>
<text top="275" left="788" width="0" height="8" font="8">8K</text>
<text top="211" left="485" width="0" height="8" font="8">Cycl</text>
<text top="195" left="485" width="0" height="8" font="8">es pe</text>
<text top="175" left="485" width="0" height="8" font="8">r </text>
<text top="170" left="485" width="0" height="8" font="8">o</text>
<text top="166" left="485" width="0" height="8" font="8">utpu</text>
<text top="150" left="485" width="0" height="8" font="8">t t</text>
<text top="143" left="485" width="0" height="8" font="8">u</text>
<text top="139" left="485" width="0" height="8" font="8">pl</text>
<text top="132" left="485" width="0" height="8" font="8">e</text>
<text top="324" left="617" width="73" height="8" font="8">Number of partitions</text>
<text top="307" left="726" width="39" height="8" font="8">Radix-best</text>
<text top="307" left="642" width="45" height="8" font="8">Independent</text>
<text top="307" left="571" width="26" height="8" font="8">Shared</text>
<text top="307" left="520" width="10" height="8" font="8">No</text>
<text top="86" left="582" width="38" height="10" font="9">partition</text>
<text top="87" left="668" width="23" height="10" font="9">build</text>
<text top="86" left="737" width="28" height="10" font="9">probe</text>
<text top="338" left="561" width="150" height="13" font="3">(b) Sun UltraSPARC T2</text>
<text top="361" left="246" width="422" height="13" font="3">Figure 3: Cycles per output tuple for the high skew dataset.</text>
<text top="388" left="212" width="28" height="13" font="3">Intel</text>
<text top="388" left="322" width="23" height="13" font="3">Sun</text>
<text top="404" left="199" width="53" height="13" font="3">Nehalem</text>
<text top="404" left="284" width="99" height="13" font="3">UltraSPARC T2</text>
<text top="420" left="141" width="21" height="13" font="3">NO</text>
<text top="420" left="205" width="40" height="13" font="3">No / 1</text>
<text top="420" left="314" width="40" height="13" font="3">No / 1</text>
<text top="437" left="143" width="18" height="13" font="3">SN</text>
<text top="437" left="191" width="69" height="13" font="3">Indep. / 16</text>
<text top="437" left="299" width="69" height="13" font="3">Indep. / 64</text>
<text top="453" left="138" width="28" height="13" font="3">L2-S</text>
<text top="453" left="183" width="85" height="13" font="3">Shared / 2048</text>
<text top="453" left="291" width="85" height="13" font="3">Shared / 2048</text>
<text top="470" left="136" width="30" height="13" font="3">L2-R</text>
<text top="470" left="186" width="79" height="13" font="3">Radix / 2048</text>
<text top="470" left="294" width="79" height="13" font="3">Radix / 2048</text>
<text top="501" left="81" width="359" height="13" font="3">Table 2: Shorthand notation and corresponding par-</text>
<text top="516" left="81" width="288" height="13" font="3">titioning strategy / number of partitions.</text>
<text top="564" left="94" width="345" height="13" font="3">By comparing Figure 1 with Figure 2, we notice that,</text>
<text top="579" left="81" width="359" height="13" font="3">when using the shared hash table (bar “No” in all graphs),</text>
<text top="595" left="81" width="359" height="13" font="3">performance actually improves in the presence of skew! On</text>
<text top="611" left="81" width="359" height="13" font="3">the other hand, the performance of the shared partitioning</text>
<text top="627" left="81" width="359" height="13" font="3">algorithm degrades rapidly with increasing skew, while the</text>
<text top="642" left="81" width="359" height="13" font="3">performance of the independent partitioning and the radix</text>
<text top="658" left="81" width="359" height="13" font="3">partitioning algorithms shows little change on the Intel Ne-</text>
<text top="674" left="81" width="359" height="13" font="3">halem and degrades on the Sun UltraSPARC T2. Mov-</text>
<text top="689" left="81" width="359" height="13" font="3">ing to Figure 3, we see that the relative performance of</text>
<text top="705" left="81" width="359" height="13" font="3">the non-partitioned join algorithm increases rapidly under</text>
<text top="721" left="81" width="359" height="13" font="3">higher skew, compared to the other algorithms. The non-</text>
<text top="736" left="81" width="359" height="13" font="3">partitioned algorithm is generally 2X faster than the other</text>
<text top="752" left="81" width="359" height="13" font="3">algorithms on the Intel Nehalem, and more than 4X faster</text>
<text top="768" left="81" width="334" height="13" font="3">than the other algorithms on the Sun UltraSPARC T2.</text>
<text top="783" left="94" width="345" height="13" font="3">To summarize these results, skew in the underlying join</text>
<text top="799" left="81" width="359" height="13" font="3">key values (data skew) manifests itself as partition size skew</text>
<text top="815" left="81" width="358" height="13" font="3">when using partitioning. For the shared partitioning algo-</text>
<text top="831" left="81" width="359" height="13" font="3">rithm, during the partition phase, skew causes latch con-</text>
<text top="846" left="81" width="359" height="13" font="3">tention on the partition with the most popular key(s). For</text>
<text top="862" left="81" width="359" height="13" font="3">all partitioning-based algorithms, during the probe phase,</text>
<text top="878" left="81" width="359" height="13" font="3">skew translates into a skewed work distribution per thread.</text>
<text top="893" left="81" width="359" height="13" font="3">Therefore, the overall join completion time is determined by</text>
<text top="909" left="81" width="359" height="13" font="3">the completion time of the partition with the most popular</text>
<text top="925" left="81" width="359" height="13" font="3">key. (We explore this behavior further in Section 4.7.1.) On</text>
<text top="940" left="81" width="359" height="13" font="3">the other hand, skew improves performance when sharing</text>
<text top="956" left="81" width="359" height="13" font="3">the hash table and not doing partitioning for two reasons.</text>
<text top="972" left="81" width="359" height="13" font="3">First, the no partitioning approach ensures an even work</text>
<text top="987" left="81" width="359" height="13" font="3">distribution per thread as all the threads are working con-</text>
<text top="1003" left="81" width="359" height="13" font="3">currently on the single partition. This greedy scheduling</text>
<text top="1019" left="81" width="359" height="13" font="3">strategy proves to be eﬀective in hiding data skew. Second,</text>
<text top="1034" left="81" width="359" height="13" font="3">performance increases because the hardware handles skew a</text>
<text top="1050" left="81" width="359" height="13" font="3">lot more eﬃciently, as skewed memory access patterns cause</text>
<text top="1066" left="81" width="192" height="13" font="3">signiﬁcantly fewer cache misses.</text>
<text top="388" left="748" width="26" height="12" font="4">TLB</text>
<text top="388" left="791" width="26" height="12" font="4">TLB</text>
<text top="401" left="597" width="36" height="12" font="4">Cycles</text>
<text top="401" left="658" width="14" height="12" font="4">L3</text>
<text top="401" left="689" width="43" height="12" font="4">Instruc-</text>
<text top="401" left="751" width="23" height="12" font="4">load</text>
<text top="401" left="790" width="27" height="12" font="4">store</text>
<text top="415" left="648" width="24" height="12" font="4">miss</text>
<text top="415" left="701" width="31" height="12" font="4">-tions</text>
<text top="415" left="750" width="24" height="12" font="4">miss</text>
<text top="415" left="793" width="24" height="12" font="4">miss</text>
<text top="429" left="532" width="49" height="12" font="4">partition</text>
<text top="429" left="626" width="6" height="12" font="4">0</text>
<text top="429" left="666" width="6" height="12" font="4">0</text>
<text top="429" left="726" width="6" height="12" font="4">0</text>
<text top="429" left="768" width="6" height="12" font="4">0</text>
<text top="429" left="811" width="6" height="12" font="4">0</text>
<text top="442" left="492" width="19" height="12" font="4">NO</text>
<text top="442" left="552" width="28" height="12" font="4">build</text>
<text top="442" left="613" width="19" height="12" font="4">322</text>
<text top="442" left="666" width="6" height="12" font="4">2</text>
<text top="442" left="703" width="29" height="12" font="4">2,215</text>
<text top="442" left="768" width="6" height="12" font="4">1</text>
<text top="442" left="811" width="6" height="12" font="4">0</text>
<text top="456" left="549" width="31" height="12" font="4">probe</text>
<text top="456" left="597" width="35" height="12" font="4">15,829</text>
<text top="456" left="653" width="19" height="12" font="4">862</text>
<text top="456" left="697" width="35" height="12" font="4">54,762</text>
<text top="456" left="755" width="19" height="12" font="4">557</text>
<text top="456" left="811" width="6" height="12" font="4">0</text>
<text top="470" left="532" width="49" height="12" font="4">partition</text>
<text top="470" left="603" width="29" height="12" font="4">3,578</text>
<text top="470" left="660" width="13" height="12" font="4">18</text>
<text top="470" left="697" width="35" height="12" font="4">29,096</text>
<text top="470" left="768" width="6" height="12" font="4">6</text>
<text top="470" left="811" width="6" height="12" font="4">2</text>
<text top="483" left="493" width="17" height="12" font="4">SN</text>
<text top="483" left="552" width="28" height="12" font="4">build</text>
<text top="483" left="613" width="19" height="12" font="4">328</text>
<text top="483" left="666" width="6" height="12" font="4">8</text>
<text top="483" left="703" width="29" height="12" font="4">2,064</text>
<text top="483" left="768" width="6" height="12" font="4">0</text>
<text top="483" left="811" width="6" height="12" font="4">0</text>
<text top="497" left="549" width="31" height="12" font="4">probe</text>
<text top="497" left="597" width="35" height="12" font="4">21,717</text>
<text top="497" left="653" width="19" height="12" font="4">866</text>
<text top="497" left="697" width="35" height="12" font="4">54,761</text>
<text top="497" left="755" width="19" height="12" font="4">505</text>
<text top="497" left="811" width="6" height="12" font="4">0</text>
<text top="511" left="532" width="49" height="12" font="4">partition</text>
<text top="511" left="597" width="35" height="12" font="4">11,778</text>
<text top="511" left="653" width="19" height="12" font="4">103</text>
<text top="511" left="697" width="35" height="12" font="4">31,117</text>
<text top="511" left="755" width="19" height="12" font="4">167</text>
<text top="511" left="798" width="19" height="12" font="4">257</text>
<text top="525" left="489" width="26" height="12" font="4">L2-S</text>
<text top="525" left="552" width="28" height="12" font="4">build</text>
<text top="525" left="613" width="19" height="12" font="4">211</text>
<text top="525" left="666" width="6" height="12" font="4">1</text>
<text top="525" left="703" width="29" height="12" font="4">2,064</text>
<text top="525" left="768" width="6" height="12" font="4">0</text>
<text top="525" left="811" width="6" height="12" font="4">0</text>
<text top="538" left="549" width="31" height="12" font="4">probe</text>
<text top="538" left="603" width="29" height="12" font="4">6,144</text>
<text top="538" left="660" width="13" height="12" font="4">35</text>
<text top="538" left="697" width="35" height="12" font="4">54,762</text>
<text top="538" left="768" width="6" height="12" font="4">1</text>
<text top="538" left="811" width="6" height="12" font="4">0</text>
<text top="552" left="532" width="49" height="12" font="4">partition</text>
<text top="552" left="603" width="29" height="12" font="4">6,343</text>
<text top="552" left="653" width="19" height="12" font="4">221</text>
<text top="552" left="697" width="35" height="12" font="4">34,241</text>
<text top="552" left="768" width="6" height="12" font="4">7</text>
<text top="552" left="798" width="19" height="12" font="4">237</text>
<text top="566" left="488" width="28" height="12" font="4">L2-R</text>
<text top="566" left="552" width="28" height="12" font="4">build</text>
<text top="566" left="613" width="19" height="12" font="4">210</text>
<text top="566" left="666" width="6" height="12" font="4">1</text>
<text top="566" left="703" width="29" height="12" font="4">2,064</text>
<text top="566" left="768" width="6" height="12" font="4">0</text>
<text top="566" left="811" width="6" height="12" font="4">0</text>
<text top="579" left="549" width="31" height="12" font="4">probe</text>
<text top="579" left="603" width="29" height="12" font="4">6,152</text>
<text top="579" left="660" width="13" height="12" font="4">36</text>
<text top="579" left="697" width="35" height="12" font="4">54,761</text>
<text top="579" left="768" width="6" height="12" font="4">1</text>
<text top="579" left="811" width="6" height="12" font="4">0</text>
<text top="608" left="475" width="359" height="13" font="3">Table 3: Performance counter averages for the uni-</text>
<text top="624" left="475" width="164" height="13" font="3">form dataset (millions).</text>
<text top="668" left="475" width="22" height="16" font="1">4.5</text>
<text top="668" left="516" width="169" height="16" font="1">Performance counters</text>
<text top="689" left="489" width="345" height="13" font="3">Due to space constraints, we focus on speciﬁc partitioning</text>
<text top="705" left="475" width="359" height="13" font="3">conﬁgurations from this section onward. We use “NO” to</text>
<text top="721" left="475" width="359" height="13" font="3">denote the no partitioning strategy where the hash table is</text>
<text top="736" left="475" width="359" height="13" font="3">shared by all threads, and we use “SN” to denote the case</text>
<text top="752" left="475" width="359" height="13" font="3">when we create as many partitions as hardware contexts</text>
<text top="768" left="475" width="359" height="13" font="3">(join threads), except we round the number of partitions up</text>
<text top="783" left="475" width="359" height="13" font="3">to the next power of two as is required for the radix par-</text>
<text top="799" left="475" width="359" height="13" font="3">titioning algorithm. We use “L2” to denote the case when</text>
<text top="815" left="475" width="359" height="13" font="3">we create partitions to ﬁt in the last level cache, appending</text>
<text top="831" left="474" width="362" height="13" font="3">“-S” when partitioning with shared output buﬀers, and “-R”</text>
<text top="846" left="475" width="359" height="13" font="3">for radix partitioning. We summarize this notation in Table</text>
<text top="862" left="475" width="359" height="13" font="3">2. Notice that the L2 numbers correspond to the best per-</text>
<text top="878" left="475" width="359" height="13" font="3">forming conﬁguration settings in the experiment with the</text>
<text top="893" left="475" width="188" height="13" font="3">uniform dataset (see Figure 1).</text>
<text top="909" left="489" width="345" height="13" font="3">We now use the hardware performance counters to un-</text>
<text top="925" left="475" width="359" height="13" font="3">derstand the characteristics of these join algorithms. In the</text>
<text top="940" left="475" width="359" height="13" font="3">interest of space, we only present our ﬁndings from a single</text>
<text top="956" left="475" width="359" height="13" font="3">architecture: the Intel Nehalem. We ﬁrst show the results</text>
<text top="972" left="475" width="359" height="13" font="3">from the uniform dataset in Table 3. Each row indicates one</text>
<text top="987" left="475" width="359" height="13" font="3">particular partitioning algorithm and join phase, and each</text>
<text top="1003" left="475" width="359" height="13" font="3">column shows a diﬀerent architectural event. First, notice</text>
<text top="1019" left="475" width="358" height="13" font="3">the code path length. It takes, on average, about 55 billion</text>
<text top="1034" left="475" width="359" height="13" font="3">instructions to complete the probe phase and an additional</text>
<text top="1050" left="475" width="358" height="13" font="3">50% to 65% of that for partitioning, depending on the al-</text>
<text top="1066" left="475" width="359" height="13" font="3">gorithm of choice. The NO algorithm pays a high cost in</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="1188" width="918">
<text top="82" left="354" width="26" height="12" font="4">TLB</text>
<text top="82" left="397" width="26" height="12" font="4">TLB</text>
<text top="95" left="202" width="36" height="12" font="4">Cycles</text>
<text top="95" left="264" width="14" height="12" font="4">L3</text>
<text top="95" left="294" width="43" height="12" font="4">Instruc-</text>
<text top="95" left="356" width="23" height="12" font="4">load</text>
<text top="95" left="396" width="27" height="12" font="4">store</text>
<text top="109" left="254" width="24" height="12" font="4">miss</text>
<text top="109" left="306" width="31" height="12" font="4">-tions</text>
<text top="109" left="356" width="24" height="12" font="4">miss</text>
<text top="109" left="399" width="24" height="12" font="4">miss</text>
<text top="123" left="137" width="49" height="12" font="4">partition</text>
<text top="123" left="231" width="6" height="12" font="4">0</text>
<text top="123" left="271" width="6" height="12" font="4">0</text>
<text top="123" left="331" width="6" height="12" font="4">0</text>
<text top="123" left="373" width="6" height="12" font="4">0</text>
<text top="123" left="416" width="6" height="12" font="4">0</text>
<text top="136" left="97" width="19" height="12" font="4">NO</text>
<text top="136" left="158" width="28" height="12" font="4">build</text>
<text top="136" left="219" width="19" height="12" font="4">323</text>
<text top="136" left="271" width="6" height="12" font="4">3</text>
<text top="136" left="309" width="29" height="12" font="4">2,215</text>
<text top="136" left="373" width="6" height="12" font="4">1</text>
<text top="136" left="416" width="6" height="12" font="4">0</text>
<text top="150" left="154" width="31" height="12" font="4">probe</text>
<text top="150" left="209" width="29" height="12" font="4">6,433</text>
<text top="150" left="265" width="13" height="12" font="4">98</text>
<text top="150" left="302" width="35" height="12" font="4">54,762</text>
<text top="150" left="361" width="19" height="12" font="4">201</text>
<text top="150" left="416" width="6" height="12" font="4">0</text>
<text top="164" left="137" width="49" height="12" font="4">partition</text>
<text top="164" left="209" width="29" height="12" font="4">3,577</text>
<text top="164" left="265" width="13" height="12" font="4">17</text>
<text top="164" left="302" width="35" height="12" font="4">29,096</text>
<text top="164" left="373" width="6" height="12" font="4">6</text>
<text top="164" left="416" width="6" height="12" font="4">1</text>
<text top="178" left="99" width="17" height="12" font="4">SN</text>
<text top="178" left="158" width="28" height="12" font="4">build</text>
<text top="178" left="219" width="19" height="12" font="4">329</text>
<text top="178" left="271" width="6" height="12" font="4">8</text>
<text top="178" left="309" width="29" height="12" font="4">2,064</text>
<text top="178" left="373" width="6" height="12" font="4">0</text>
<text top="178" left="416" width="6" height="12" font="4">0</text>
<text top="191" left="154" width="31" height="12" font="4">probe</text>
<text top="191" left="202" width="35" height="12" font="4">13,241</text>
<text top="191" left="265" width="13" height="12" font="4">61</text>
<text top="191" left="302" width="35" height="12" font="4">54,761</text>
<text top="191" left="367" width="13" height="12" font="4">80</text>
<text top="191" left="416" width="6" height="12" font="4">0</text>
<text top="205" left="137" width="49" height="12" font="4">partition</text>
<text top="205" left="202" width="35" height="12" font="4">36,631</text>
<text top="205" left="265" width="13" height="12" font="4">79</text>
<text top="205" left="302" width="35" height="12" font="4">34,941</text>
<text top="205" left="367" width="13" height="12" font="4">67</text>
<text top="205" left="404" width="19" height="12" font="4">106</text>
<text top="219" left="94" width="26" height="12" font="4">L2-S</text>
<text top="219" left="158" width="28" height="12" font="4">build</text>
<text top="219" left="219" width="19" height="12" font="4">210</text>
<text top="219" left="271" width="6" height="12" font="4">5</text>
<text top="219" left="309" width="29" height="12" font="4">2,064</text>
<text top="219" left="373" width="6" height="12" font="4">0</text>
<text top="219" left="416" width="6" height="12" font="4">0</text>
<text top="232" left="154" width="31" height="12" font="4">probe</text>
<text top="232" left="209" width="29" height="12" font="4">8,024</text>
<text top="232" left="265" width="13" height="12" font="4">13</text>
<text top="232" left="302" width="35" height="12" font="4">54,762</text>
<text top="232" left="373" width="6" height="12" font="4">1</text>
<text top="232" left="416" width="6" height="12" font="4">0</text>
<text top="246" left="137" width="49" height="12" font="4">partition</text>
<text top="246" left="209" width="29" height="12" font="4">5,344</text>
<text top="246" left="259" width="19" height="12" font="4">178</text>
<text top="246" left="302" width="35" height="12" font="4">34,241</text>
<text top="246" left="373" width="6" height="12" font="4">5</text>
<text top="246" left="410" width="13" height="12" font="4">72</text>
<text top="260" left="93" width="28" height="12" font="4">L2-R</text>
<text top="260" left="158" width="28" height="12" font="4">build</text>
<text top="260" left="219" width="19" height="12" font="4">209</text>
<text top="260" left="271" width="6" height="12" font="4">4</text>
<text top="260" left="309" width="29" height="12" font="4">2,064</text>
<text top="260" left="373" width="6" height="12" font="4">0</text>
<text top="260" left="416" width="6" height="12" font="4">0</text>
<text top="273" left="154" width="31" height="12" font="4">probe</text>
<text top="273" left="209" width="29" height="12" font="4">8,052</text>
<text top="273" left="265" width="13" height="12" font="4">13</text>
<text top="273" left="302" width="35" height="12" font="4">54,761</text>
<text top="273" left="373" width="6" height="12" font="4">1</text>
<text top="273" left="416" width="6" height="12" font="4">0</text>
<text top="303" left="81" width="359" height="13" font="3">Table 4: Performance counter averages for the high</text>
<text top="318" left="81" width="165" height="13" font="3">skew dataset (millions).</text>
<text top="360" left="81" width="359" height="13" font="3">terms of the L3 cache misses during the probe phase. The</text>
<text top="375" left="81" width="359" height="13" font="3">partitioning phase of the SN algorithm is fast but fails to</text>
<text top="391" left="81" width="359" height="13" font="3">contain the memory reference patterns that arise during the</text>
<text top="407" left="81" width="359" height="13" font="3">probe phase in the cache. The L2-S algorithm manages to</text>
<text top="422" left="81" width="359" height="13" font="3">minimize these memory references, but incurs a high L3 and</text>
<text top="438" left="81" width="359" height="13" font="3">TLB miss ratio during the partition phase compared to the</text>
<text top="454" left="81" width="359" height="13" font="3">NO and SN algorithms. The L2-R algorithm uses multiple</text>
<text top="470" left="81" width="359" height="13" font="3">passes to partition the input and carefully controls the L3</text>
<text top="485" left="81" width="359" height="13" font="3">and TLB misses during these phases. Once the cache-sized</text>
<text top="501" left="81" width="359" height="13" font="3">partitions have been created, we see that both the L2-S and</text>
<text top="517" left="81" width="359" height="13" font="3">L2-R algorithms avoid incurring many L3 and TLB misses</text>
<text top="532" left="81" width="359" height="13" font="3">during the probe phase. In general, we see fewer cache and</text>
<text top="548" left="81" width="359" height="13" font="3">TLB misses across all algorithms when adding skew (in Ta-</text>
<text top="564" left="81" width="38" height="13" font="3">ble 4).</text>
<text top="579" left="94" width="345" height="13" font="3">Unfortunately, interpreting performance counters is much</text>
<text top="595" left="81" width="359" height="13" font="3">more challenging with modern multi-core processors and will</text>
<text top="611" left="81" width="359" height="13" font="3">likely get worse. Processors have become a lot more com-</text>
<text top="627" left="81" width="359" height="13" font="3">plex over the last ten years, yet the events that counters</text>
<text top="642" left="81" width="358" height="13" font="3">capture have hardly changed. This trend causes a grow-</text>
<text top="658" left="81" width="359" height="13" font="3">ing gap between the high-level algorithmic insights the user</text>
<text top="674" left="81" width="359" height="13" font="3">expects and the speciﬁc causes that trigger some proces-</text>
<text top="689" left="81" width="359" height="13" font="3">sor state that the performance counters can capture. In a</text>
<text top="705" left="81" width="359" height="13" font="3">uniprocessor, for example, a cache miss is an indication that</text>
<text top="721" left="81" width="359" height="13" font="3">the working set exceeds the cache’s capacity. The penalty</text>
<text top="736" left="81" width="359" height="13" font="3">is bringing the data from memory, an operation the costs</text>
<text top="752" left="81" width="359" height="13" font="3">hundreds of cycles. However, in a multi-core processor, a</text>
<text top="768" left="81" width="358" height="13" font="3">memory load might miss in the cache because the operation</text>
<text top="783" left="81" width="359" height="13" font="3">touches memory that some other core has just modiﬁed. The</text>
<text top="799" left="81" width="359" height="13" font="3">penalty in this case is looking in some other cache for the</text>
<text top="815" left="81" width="359" height="13" font="3">data. Although a neighboring cache lookup can be ten or a</text>
<text top="831" left="81" width="359" height="13" font="3">hundred times faster than bringing the data from memory,</text>
<text top="846" left="81" width="359" height="13" font="3">both scenarios will simply increment the cache miss counter</text>
<text top="862" left="81" width="233" height="13" font="3">and not record the cause of this event.</text>
<text top="878" left="94" width="345" height="13" font="3">To illustrate this point, let’s turn our attention to a case in</text>
<text top="893" left="81" width="359" height="13" font="3">Table 3 where the performance counter results can be mis-</text>
<text top="909" left="81" width="359" height="13" font="3">leading: The probe phase of the SN algorithm has slightly</text>
<text top="925" left="81" width="359" height="13" font="3">fewer L3 and TLB misses than the probe phase of the NO</text>
<text top="940" left="81" width="359" height="13" font="3">algorithm and equal path length, so the probe phase of the</text>
<text top="956" left="81" width="359" height="13" font="3">SN algorithm should be comparable or faster than probe</text>
<text top="972" left="81" width="359" height="13" font="3">phase of the NO algorithm. However, the probe phase of</text>
<text top="987" left="81" width="359" height="13" font="3">the NO algorithm is almost 25% faster! Another issue is</text>
<text top="1003" left="81" width="359" height="13" font="3">latch contention, which causes neither L3 cache misses nor</text>
<text top="1019" left="81" width="359" height="13" font="3">TLB misses, and therefore is not reported in the perfor-</text>
<text top="1034" left="81" width="359" height="13" font="3">mance counters. For example, when comparing the uniform</text>
<text top="1050" left="81" width="359" height="13" font="3">and high skew numbers for the L2-S algorithm, the number</text>
<text top="1066" left="81" width="359" height="13" font="3">of the L3 cache misses during the high skew experiment is</text>
<text top="300" left="549" width="5" height="8" font="8">0</text>
<text top="264" left="549" width="5" height="8" font="8">2</text>
<text top="227" left="549" width="5" height="8" font="8">4</text>
<text top="191" left="549" width="5" height="8" font="8">6</text>
<text top="154" left="549" width="5" height="8" font="8">8</text>
<text top="118" left="544" width="9" height="8" font="8">10</text>
<text top="81" left="544" width="9" height="8" font="8">12</text>
<text top="310" left="556" width="5" height="8" font="8">0</text>
<text top="310" left="592" width="5" height="8" font="8">2</text>
<text top="310" left="629" width="5" height="8" font="8">4</text>
<text top="310" left="665" width="5" height="8" font="8">6</text>
<text top="310" left="702" width="5" height="8" font="8">8</text>
<text top="310" left="736" width="9" height="8" font="8">10</text>
<text top="310" left="772" width="9" height="8" font="8">12</text>
<text top="205" left="533" width="0" height="8" font="8">S</text>
<text top="199" left="533" width="0" height="8" font="8">pe</text>
<text top="190" left="533" width="0" height="8" font="8">ed</text>
<text top="181" left="533" width="0" height="8" font="8">up</text>
<text top="325" left="634" width="67" height="8" font="8">Number of threads</text>
<text top="91" left="568" width="12" height="8" font="8">NO</text>
<text top="101" left="569" width="11" height="8" font="8">SN</text>
<text top="110" left="563" width="17" height="8" font="8">L2-S</text>
<text top="120" left="563" width="18" height="8" font="8">L2-R</text>
<text top="352" left="475" width="359" height="13" font="3">Figure 4: Speedup over single threaded execution,</text>
<text top="367" left="475" width="115" height="13" font="3">uniform dataset.</text>
<text top="409" left="475" width="359" height="13" font="3">30% lower than the number of the cache misses observed</text>
<text top="425" left="475" width="359" height="13" font="3">during the uniform experiment. However, partitioning per-</text>
<text top="440" left="475" width="359" height="13" font="3">formance worsens by more than 3X when creating shared</text>
<text top="456" left="475" width="165" height="13" font="3">partitions under high skew!</text>
<text top="472" left="489" width="345" height="13" font="3">The performance counters don’t provide clean insights</text>
<text top="487" left="475" width="359" height="13" font="3">into why the non-partitioned algorithm exhibits similar or</text>
<text top="503" left="475" width="358" height="13" font="3">better performance than the other cache-eﬃcient algorithms</text>
<text top="519" left="475" width="358" height="13" font="3">across all datasets. Although a cycle breakdown is still fea-</text>
<text top="534" left="475" width="359" height="13" font="3">sible at a macroscopic level where the assumption of no con-</text>
<text top="550" left="475" width="359" height="13" font="3">tention holds (for example as in Ailamaki et al. [1]), this ex-</text>
<text top="566" left="475" width="359" height="13" font="3">periment reveals that blindly assigning ﬁxed cycle penalties</text>
<text top="582" left="475" width="351" height="13" font="3">to architectural events can lead to misleading conclusions.</text>
<text top="620" left="475" width="22" height="16" font="1">4.6</text>
<text top="620" left="516" width="151" height="16" font="1">Speedup from SMT</text>
<text top="642" left="489" width="350" height="13" font="3">Modern processors improve the overall eﬃciency with hard-</text>
<text top="658" left="475" width="359" height="13" font="3">ware multithreading. Simultaneous multi-threading (SMT)</text>
<text top="674" left="475" width="359" height="13" font="3">permits multiple independent threads of execution to better</text>
<text top="689" left="475" width="359" height="13" font="3">utilize the resources provided by modern processor architec-</text>
<text top="705" left="475" width="358" height="13" font="3">tures. We now evaluate the impact of SMT on the hash join</text>
<text top="721" left="475" width="67" height="13" font="3">algorithms.</text>
<text top="736" left="489" width="345" height="13" font="3">We ﬁrst show a speedup experiment for the Intel Nehalem</text>
<text top="752" left="475" width="359" height="13" font="3">on the uniform dataset in Figure 4. We start by dedicating</text>
<text top="768" left="475" width="359" height="13" font="3">each thread to a core, and once we exceed the number of</text>
<text top="783" left="475" width="359" height="13" font="3">available physical cores (six for our Intel Nehalem), we then</text>
<text top="799" left="475" width="359" height="13" font="3">start assigning threads in a round-robin fashion to the avail-</text>
<text top="815" left="475" width="359" height="13" font="3">able hardware contexts. We observe that the algorithms be-</text>
<text top="831" left="475" width="359" height="13" font="3">have very diﬀerently when some cores are idle (fewer than six</text>
<text top="846" left="475" width="359" height="13" font="3">threads) versus in the SMT region (more than six threads).</text>
<text top="862" left="475" width="359" height="13" font="3">With fewer than six threads all the algorithms scale linearly,</text>
<text top="878" left="475" width="359" height="13" font="3">and the NO algorithm has optimal speedup. With more</text>
<text top="893" left="475" width="359" height="13" font="3">than six threads, the NO algorithm continues to scale, be-</text>
<text top="909" left="475" width="359" height="13" font="3">coming almost 11X faster than the single-threaded version</text>
<text top="925" left="475" width="359" height="13" font="3">when using all available contexts. The partitioning-based</text>
<text top="940" left="475" width="359" height="13" font="3">algorithms SN, L2-S and L2-R, however, do not exhibit this</text>
<text top="956" left="475" width="359" height="13" font="3">behavior. The speedup curve for these three algorithms in</text>
<text top="972" left="475" width="359" height="13" font="3">the SMT region either ﬂattens completely (SN algorithm), or</text>
<text top="987" left="475" width="359" height="13" font="3">increases at a reduced rate (L2-R algorithm) than the non-</text>
<text top="1003" left="475" width="359" height="13" font="3">SMT region. In fact, performance drops for all partitioning</text>
<text top="1019" left="475" width="359" height="13" font="3">algorithms for seven threads because of load imbalance: a</text>
<text top="1034" left="475" width="359" height="13" font="3">single core has to do the work for two threads. (This imbal-</text>
<text top="1050" left="475" width="359" height="13" font="3">ance can be ameliorated through load balancing, a technique</text>
<text top="1066" left="475" width="200" height="13" font="3">that we explore in Section 4.7.1.)</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="1188" width="918">
<text top="82" left="205" width="58" height="13" font="3">Uniform</text>
<text top="98" left="167" width="56" height="13" font="3">6 threads</text>
<text top="98" left="239" width="63" height="13" font="3">12 threads</text>
<text top="98" left="321" width="80" height="13" font="3">Improvement</text>
<text top="114" left="125" width="21" height="13" font="3">NO</text>
<text top="114" left="179" width="31" height="13" font="3">28.23</text>
<text top="114" left="255" width="31" height="13" font="3">16.15</text>
<text top="114" left="343" width="35" height="13" font="3">1.75X</text>
<text top="131" left="126" width="18" height="13" font="3">SN</text>
<text top="131" left="179" width="31" height="13" font="3">34.04</text>
<text top="131" left="255" width="31" height="13" font="3">25.62</text>
<text top="131" left="343" width="35" height="13" font="3">1.33X</text>
<text top="147" left="121" width="28" height="13" font="3">L2-S</text>
<text top="147" left="179" width="31" height="13" font="3">19.27</text>
<text top="147" left="255" width="31" height="13" font="3">18.13</text>
<text top="147" left="343" width="35" height="13" font="3">1.06X</text>
<text top="164" left="120" width="30" height="13" font="3">L2-R</text>
<text top="164" left="179" width="31" height="13" font="3">14.46</text>
<text top="164" left="255" width="31" height="13" font="3">12.71</text>
<text top="164" left="343" width="35" height="13" font="3">1.14X</text>
<text top="183" left="198" width="72" height="13" font="3">High skew</text>
<text top="199" left="167" width="56" height="13" font="3">6 threads</text>
<text top="199" left="239" width="63" height="13" font="3">12 threads</text>
<text top="199" left="321" width="80" height="13" font="3">Improvement</text>
<text top="215" left="125" width="21" height="13" font="3">NO</text>
<text top="215" left="182" width="24" height="13" font="3">9.34</text>
<text top="215" left="258" width="24" height="13" font="3">6.76</text>
<text top="215" left="343" width="35" height="13" font="3">1.38X</text>
<text top="231" left="126" width="18" height="13" font="3">SN</text>
<text top="231" left="179" width="31" height="13" font="3">19.50</text>
<text top="231" left="255" width="31" height="13" font="3">17.15</text>
<text top="231" left="343" width="35" height="13" font="3">1.14X</text>
<text top="248" left="121" width="28" height="13" font="3">L2-S</text>
<text top="248" left="179" width="31" height="13" font="3">38.37</text>
<text top="248" left="255" width="31" height="13" font="3">44.87</text>
<text top="248" left="343" width="35" height="13" font="3">0.86X</text>
<text top="264" left="120" width="30" height="13" font="3">L2-R</text>
<text top="264" left="179" width="31" height="13" font="3">15.04</text>
<text top="264" left="255" width="31" height="13" font="3">13.61</text>
<text top="264" left="343" width="35" height="13" font="3">1.11X</text>
<text top="296" left="81" width="359" height="13" font="3">Table 5: Simultaneous multi-threading experiment</text>
<text top="311" left="81" width="359" height="13" font="3">on the Intel Nehalem, showing billions of cycles to</text>
<text top="327" left="81" width="297" height="13" font="3">join completion and relative improvement.</text>
<text top="370" left="205" width="58" height="13" font="3">Uniform</text>
<text top="386" left="167" width="56" height="13" font="3">8 threads</text>
<text top="386" left="239" width="63" height="13" font="3">64 threads</text>
<text top="386" left="321" width="80" height="13" font="3">Improvement</text>
<text top="402" left="125" width="21" height="13" font="3">NO</text>
<text top="402" left="179" width="31" height="13" font="3">37.30</text>
<text top="402" left="255" width="31" height="13" font="3">12.64</text>
<text top="402" left="343" width="35" height="13" font="3">2.95X</text>
<text top="419" left="126" width="18" height="13" font="3">SN</text>
<text top="419" left="179" width="31" height="13" font="3">55.70</text>
<text top="419" left="255" width="31" height="13" font="3">22.25</text>
<text top="419" left="343" width="35" height="13" font="3">2.50X</text>
<text top="435" left="121" width="28" height="13" font="3">L2-S</text>
<text top="435" left="179" width="31" height="13" font="3">51.62</text>
<text top="435" left="255" width="31" height="13" font="3">23.86</text>
<text top="435" left="343" width="35" height="13" font="3">2.16X</text>
<text top="452" left="120" width="30" height="13" font="3">L2-R</text>
<text top="452" left="179" width="31" height="13" font="3">46.62</text>
<text top="452" left="255" width="31" height="13" font="3">18.88</text>
<text top="452" left="343" width="35" height="13" font="3">2.47X</text>
<text top="471" left="198" width="72" height="13" font="3">High skew</text>
<text top="487" left="167" width="56" height="13" font="3">8 threads</text>
<text top="487" left="239" width="63" height="13" font="3">64 threads</text>
<text top="487" left="321" width="80" height="13" font="3">Improvement</text>
<text top="503" left="125" width="21" height="13" font="3">NO</text>
<text top="503" left="179" width="31" height="13" font="3">23.92</text>
<text top="503" left="255" width="31" height="13" font="3">11.67</text>
<text top="503" left="343" width="35" height="13" font="3">2.05X</text>
<text top="519" left="126" width="18" height="13" font="3">SN</text>
<text top="519" left="179" width="31" height="13" font="3">70.52</text>
<text top="519" left="255" width="31" height="13" font="3">49.54</text>
<text top="519" left="343" width="35" height="13" font="3">1.42X</text>
<text top="536" left="121" width="28" height="13" font="3">L2-S</text>
<text top="536" left="179" width="31" height="13" font="3">73.91</text>
<text top="536" left="251" width="38" height="13" font="3">221.01</text>
<text top="536" left="343" width="35" height="13" font="3">0.33X</text>
<text top="552" left="120" width="30" height="13" font="3">L2-R</text>
<text top="552" left="179" width="31" height="13" font="3">66.01</text>
<text top="552" left="255" width="31" height="13" font="3">43.16</text>
<text top="552" left="343" width="35" height="13" font="3">1.53X</text>
<text top="583" left="81" width="359" height="13" font="3">Table 6: Simultaneous multi-threading experiment</text>
<text top="599" left="81" width="359" height="13" font="3">on the Sun UltraSPARC T2, showing billions of cy-</text>
<text top="615" left="81" width="346" height="13" font="3">cles to join completion and relative improvement.</text>
<text top="658" left="94" width="345" height="13" font="3">We summarize the beneﬁt of SMT in Table 5 for the In-</text>
<text top="674" left="81" width="359" height="13" font="3">tel architecture, and in Table 6 for the Sun architecture.</text>
<text top="689" left="81" width="359" height="13" font="3">For the Intel Nehalem and the uniform dataset, the NO al-</text>
<text top="705" left="81" width="358" height="13" font="3">gorithm beneﬁts signiﬁcantly from SMT, becoming 1.75X</text>
<text top="721" left="81" width="359" height="13" font="3">faster. This algorithm is not optimized for cache perfor-</text>
<text top="736" left="81" width="358" height="13" font="3">mance, and as seen in Section 4.5, causes many cache misses.</text>
<text top="752" left="81" width="359" height="13" font="3">As a result, it provides more opportunities for SMT to ef-</text>
<text top="768" left="81" width="359" height="13" font="3">ﬁciently overlap the memory accesses. On the other hand,</text>
<text top="783" left="81" width="359" height="13" font="3">the other three algorithms are optimized for cache perfor-</text>
<text top="799" left="81" width="359" height="13" font="3">mance to diﬀerent degrees. Their computation is a large</text>
<text top="815" left="81" width="359" height="13" font="3">fraction of the total execution time, therefore they do not</text>
<text top="831" left="81" width="359" height="13" font="3">beneﬁt signiﬁcantly from using SMT. In addition, we notice</text>
<text top="846" left="81" width="359" height="13" font="3">that the NO algorithm is around 2X slower than the L2-R</text>
<text top="862" left="81" width="359" height="13" font="3">algorithm without SMT, but its performance increases to</text>
<text top="878" left="81" width="353" height="13" font="3">almost match the L2-R algorithm performance with SMT.</text>
<text top="893" left="94" width="345" height="13" font="3">For the Sun UltraSPARC T2, the NO algorithm also ben-</text>
<text top="909" left="81" width="359" height="13" font="3">eﬁts the most from SMT. In this architecture the code path</text>
<text top="925" left="81" width="359" height="13" font="3">length (i.e. instructions executed) has a direct impact on the</text>
<text top="940" left="81" width="359" height="13" font="3">join completion time, and therefore the NO algorithm per-</text>
<text top="956" left="81" width="359" height="13" font="3">forms best both with and without SMT. As the Sun machine</text>
<text top="972" left="81" width="358" height="13" font="3">cannot exploit instruction parallelism at all, we see increased</text>
<text top="987" left="81" width="330" height="13" font="3">beneﬁts from SMT compared to the Intel architecture.</text>
<text top="1003" left="94" width="345" height="13" font="3">When comparing the high skew dataset with the uniform</text>
<text top="1019" left="81" width="359" height="13" font="3">dataset across both architectures, we see that the improve-</text>
<text top="1034" left="81" width="359" height="13" font="3">ment of SMT is reduced. The skewed key distribution in-</text>
<text top="1050" left="81" width="359" height="13" font="3">curs fewer cache misses, therefore SMT loses opportunities</text>
<text top="1066" left="81" width="193" height="13" font="3">to hide processor pipeline stalls.</text>
<text top="83" left="475" width="22" height="16" font="1">4.7</text>
<text top="83" left="516" width="125" height="16" font="1">Synchronization</text>
<text top="105" left="489" width="350" height="13" font="3">Synchronization is used in multithreaded programs to guar-</text>
<text top="121" left="475" width="358" height="13" font="3">antee the consistency of shared data structures. In our join</text>
<text top="136" left="475" width="359" height="13" font="3">implementations, we use barrier synchronization when all</text>
<text top="152" left="475" width="359" height="13" font="3">the threads wait for tasks to be completed before they can</text>
<text top="168" left="475" width="359" height="13" font="3">proceed to the next task. (For example, at the end of each</text>
<text top="183" left="475" width="359" height="13" font="3">pass of the radix partition phase, each thread has to wait</text>
<text top="199" left="475" width="359" height="13" font="3">until all other threads complete before proceeding.) In this</text>
<text top="215" left="475" width="359" height="13" font="3">section, we study the eﬀect of barrier synchronization on</text>
<text top="231" left="475" width="359" height="13" font="3">the performance of the hash join algorithm. In the interest</text>
<text top="246" left="475" width="359" height="13" font="3">of space, we only present results for the Intel Nehalem ma-</text>
<text top="262" left="475" width="359" height="13" font="3">chine. Since the radix partitioning algorithm wins over the</text>
<text top="278" left="475" width="359" height="13" font="3">other partitioning algorithms across all datasets, our discus-</text>
<text top="293" left="475" width="359" height="13" font="3">sion only focuses on results for the non-partitioned algorithm</text>
<text top="309" left="475" width="307" height="13" font="3">(NO) and the radix partitioning algorithm (L2-R).</text>
<text top="325" left="489" width="345" height="13" font="3">Synchronization has little impact on the non-partitioned</text>
<text top="340" left="475" width="359" height="13" font="3">(NO) algorithm for both the uniform and the high skew</text>
<text top="356" left="475" width="359" height="13" font="3">datasets, regardless of the number of threads that are run-</text>
<text top="372" left="475" width="359" height="13" font="3">ning. The reason for this behavior is the simplicity of the</text>
<text top="387" left="475" width="359" height="13" font="3">NO algorithm. First, there is no partition phase at all, and</text>
<text top="403" left="475" width="359" height="13" font="3">each thread can proceed independently in the probe phase.</text>
<text top="419" left="475" width="359" height="13" font="3">Therefore synchronization is only necessary during the build</text>
<text top="435" left="475" width="359" height="13" font="3">phase, a phase that takes less than 2% of the total time (see</text>
<text top="450" left="475" width="359" height="13" font="3">Figure 1). Second, by dispensing with partitioning, this</text>
<text top="466" left="475" width="359" height="13" font="3">algorithm ensures an even distribution of work across the</text>
<text top="481" left="475" width="359" height="13" font="3">threads, as all the threads are working concurrently on the</text>
<text top="497" left="475" width="148" height="13" font="3">single shared hash table.</text>
<text top="513" left="489" width="345" height="13" font="3">We now turn our attention to the radix partitioning al-</text>
<text top="529" left="475" width="359" height="13" font="3">gorithm, and break down the time spent by each thread.</text>
<text top="544" left="475" width="359" height="13" font="3">Unlike the non-partitioned algorithm, the radix partitioning</text>
<text top="560" left="475" width="359" height="13" font="3">algorithm is signiﬁcantly impacted by synchronization on</text>
<text top="576" left="475" width="359" height="13" font="3">both the uniform and the high skew datasets. Figure 5(a)</text>
<text top="591" left="475" width="359" height="13" font="3">shows the time breakdown for the L2-R algorithm when run-</text>
<text top="607" left="475" width="359" height="13" font="3">ning 12 threads on the Intel Nehalem machine with the high</text>
<text top="623" left="475" width="359" height="13" font="3">skew dataset. Each histogram in this ﬁgure represents the</text>
<text top="638" left="475" width="359" height="13" font="3">execution ﬂow of a thread. The vertical axis can be viewed</text>
<text top="654" left="475" width="359" height="13" font="3">as a time axis (in machine cycles). White rectangles in these</text>
<text top="670" left="475" width="359" height="13" font="3">histograms represent tasks, the position of each rectangle in-</text>
<text top="686" left="475" width="359" height="13" font="3">dicates the beginning time of the task, and the height repre-</text>
<text top="701" left="475" width="359" height="13" font="3">sents the completion time of this task for each thread. The</text>
<text top="717" left="475" width="359" height="13" font="3">gray rectangles represent the waiting time that is incurred</text>
<text top="733" left="475" width="359" height="13" font="3">by a thread that completes its task but needs to synchro-</text>
<text top="748" left="475" width="359" height="13" font="3">nize with the other threads before continuing. In the radix</text>
<text top="764" left="475" width="359" height="13" font="3">join algorithm, we can see ﬁve expensive operations that are</text>
<text top="780" left="475" width="359" height="13" font="3">synchronized through barriers: (1) computing the thread-</text>
<text top="795" left="475" width="359" height="13" font="3">private histogram, (2) computing the global histogram, (3)</text>
<text top="811" left="475" width="359" height="13" font="3">doing radix partitioning, (4) building a hash table for each</text>
<text top="827" left="475" width="359" height="13" font="3">partition of the relation R, and (5) probing each hash table</text>
<text top="842" left="475" width="359" height="13" font="3">with a partition from the relation S. The synchronization</text>
<text top="858" left="475" width="359" height="13" font="3">cost of the radix partitioning algorithm accounts for nearly</text>
<text top="874" left="475" width="333" height="13" font="3">half of the total join completion time for some threads.</text>
<text top="890" left="489" width="345" height="13" font="3">The synchronization cost is so high under skew primar-</text>
<text top="905" left="475" width="359" height="13" font="3">ily because it is hard to statically divide work items into</text>
<text top="921" left="475" width="359" height="13" font="3">equally-sized subtasks. As a result, faster threads have to</text>
<text top="937" left="475" width="359" height="13" font="3">wait for slower threads. For example, if threads are stat-</text>
<text top="952" left="475" width="359" height="13" font="3">ically assigned to work on partitions in the probe phase,</text>
<text top="968" left="475" width="359" height="13" font="3">the distribution of the work assigned to the threads will in-</text>
<text top="984" left="475" width="359" height="13" font="3">variably also be skewed. Thus, the thread processing the</text>
<text top="999" left="475" width="359" height="13" font="3">partition with the most popular key becomes a bottleneck</text>
<text top="1015" left="475" width="359" height="13" font="3">and the overall completion time is determined by the com-</text>
<text top="1031" left="475" width="359" height="13" font="3">pletion time of the partition with the most popular keys. In</text>
<text top="1047" left="475" width="172" height="13" font="3">Figure 5(a), this is thread 3.</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="1188" width="918">
<text top="311" left="142" width="5" height="8" font="8">0</text>
<text top="282" left="142" width="5" height="8" font="8">2</text>
<text top="253" left="142" width="5" height="8" font="8">4</text>
<text top="223" left="142" width="5" height="8" font="8">6</text>
<text top="194" left="142" width="5" height="8" font="8">8</text>
<text top="165" left="138" width="9" height="8" font="8">10</text>
<text top="136" left="138" width="9" height="8" font="8">12</text>
<text top="107" left="138" width="9" height="8" font="8">14</text>
<text top="321" left="171" width="5" height="8" font="8">1</text>
<text top="321" left="192" width="5" height="8" font="8">2</text>
<text top="321" left="214" width="5" height="8" font="8">3</text>
<text top="321" left="235" width="5" height="8" font="8">4</text>
<text top="321" left="257" width="5" height="8" font="8">5</text>
<text top="321" left="278" width="5" height="8" font="8">6</text>
<text top="321" left="300" width="5" height="8" font="8">7</text>
<text top="321" left="321" width="5" height="8" font="8">8</text>
<text top="321" left="343" width="5" height="8" font="8">9</text>
<text top="321" left="362" width="9" height="8" font="8">10</text>
<text top="321" left="383" width="8" height="8" font="8">11</text>
<text top="321" left="405" width="9" height="8" font="8">12</text>
<text top="228" left="127" width="0" height="8" font="8">Cycl</text>
<text top="212" left="127" width="0" height="8" font="8">es (</text>
<text top="198" left="127" width="0" height="8" font="8">bi</text>
<text top="192" left="127" width="0" height="8" font="8">llio</text>
<text top="182" left="127" width="0" height="8" font="8">ns)</text>
<text top="335" left="273" width="36" height="8" font="8">Thread ID</text>
<text top="81" left="225" width="23" height="10" font="9">work</text>
<text top="81" left="308" width="19" height="10" font="9">wait</text>
<text top="348" left="210" width="132" height="13" font="3">(a) High skew dataset</text>
<text top="311" left="500" width="5" height="8" font="8">0</text>
<text top="282" left="500" width="5" height="8" font="8">2</text>
<text top="253" left="500" width="5" height="8" font="8">4</text>
<text top="223" left="500" width="5" height="8" font="8">6</text>
<text top="194" left="500" width="5" height="8" font="8">8</text>
<text top="165" left="496" width="9" height="8" font="8">10</text>
<text top="136" left="496" width="9" height="8" font="8">12</text>
<text top="107" left="496" width="9" height="8" font="8">14</text>
<text top="321" left="529" width="5" height="8" font="8">1</text>
<text top="321" left="550" width="5" height="8" font="8">2</text>
<text top="321" left="571" width="5" height="8" font="8">3</text>
<text top="321" left="593" width="5" height="8" font="8">4</text>
<text top="321" left="614" width="5" height="8" font="8">5</text>
<text top="321" left="636" width="5" height="8" font="8">6</text>
<text top="321" left="657" width="5" height="8" font="8">7</text>
<text top="321" left="679" width="5" height="8" font="8">8</text>
<text top="321" left="700" width="5" height="8" font="8">9</text>
<text top="321" left="720" width="9" height="8" font="8">10</text>
<text top="321" left="741" width="8" height="8" font="8">11</text>
<text top="321" left="762" width="9" height="8" font="8">12</text>
<text top="228" left="484" width="0" height="8" font="8">Cycl</text>
<text top="212" left="484" width="0" height="8" font="8">es (</text>
<text top="198" left="484" width="0" height="8" font="8">bi</text>
<text top="192" left="484" width="0" height="8" font="8">llio</text>
<text top="182" left="484" width="0" height="8" font="8">ns)</text>
<text top="335" left="631" width="36" height="8" font="8">Thread ID</text>
<text top="81" left="583" width="23" height="10" font="9">work</text>
<text top="81" left="666" width="19" height="10" font="9">wait</text>
<text top="348" left="509" width="249" height="13" font="3">(b) High skew dataset with work stealing</text>
<text top="370" left="303" width="309" height="13" font="3">Figure 5: Time breakdown of the radix join.</text>
<text top="400" left="85" width="33" height="15" font="7">4.7.1</text>
<text top="400" left="134" width="104" height="15" font="7">Load balancing</text>
<text top="420" left="94" width="345" height="13" font="3">If static work allocation is the problem, then how would</text>
<text top="436" left="81" width="359" height="13" font="3">the radix join algorithm perform under a dynamic work al-</text>
<text top="452" left="81" width="359" height="13" font="3">location policy and highly skewed input? To answer this</text>
<text top="467" left="81" width="359" height="13" font="3">question, we tweaked the join algorithm to allow the faster</text>
<text top="483" left="81" width="359" height="13" font="3">threads that have completed their probe phase to steal work</text>
<text top="499" left="81" width="359" height="13" font="3">from other slower threads. In our implementation, the unit</text>
<text top="514" left="81" width="359" height="13" font="3">of work is a single partition. In doing so, we slightly increase</text>
<text top="530" left="81" width="359" height="13" font="3">the synchronization cost because work queues need to now</text>
<text top="546" left="81" width="351" height="13" font="3">be protected with latches, but we balance the load better.</text>
<text top="561" left="94" width="345" height="13" font="3">In Figure 5(b) we plot the breakdown of the radix par-</text>
<text top="577" left="81" width="359" height="13" font="3">titioning algorithm (L2-R) using this work stealing policy</text>
<text top="593" left="81" width="359" height="13" font="3">when running on the Intel Nehalem machine with the high</text>
<text top="609" left="81" width="359" height="13" font="3">skew dataset. Although the work is now balanced almost</text>
<text top="624" left="81" width="359" height="13" font="3">perfectly for the smaller partitions, the partitions with the</text>
<text top="640" left="81" width="359" height="13" font="3">most popular keys are still a bottleneck. In the high skew</text>
<text top="656" left="81" width="359" height="13" font="3">dataset, the most popular key appears 22% of the time, and</text>
<text top="671" left="81" width="359" height="13" font="3">thread 3 in this case has been assigned only a single par-</text>
<text top="687" left="81" width="359" height="13" font="3">tition which happened to correspond to the most popular</text>
<text top="703" left="81" width="359" height="13" font="3">key. In comparison, for this particular experiment, the NO</text>
<text top="718" left="81" width="359" height="13" font="3">algorithm can complete the join in under 7 billion cycles</text>
<text top="734" left="81" width="359" height="13" font="3">(Table 4), and hence is 1.9X faster. An interesting area for</text>
<text top="750" left="81" width="359" height="13" font="3">future work is load balancing techniques that permit work</text>
<text top="766" left="81" width="359" height="13" font="3">stealing at a ﬁner granularity than an entire partition with</text>
<text top="781" left="81" width="206" height="13" font="3">a reasonable synchronization cost.</text>
<text top="797" left="94" width="345" height="13" font="3">To summarize, under skew, a load balancing technique</text>
<text top="813" left="81" width="359" height="13" font="3">improves the performance of the probe phase but does not</text>
<text top="828" left="81" width="359" height="13" font="3">address the inherent ineﬃciency of all the partitioning-based</text>
<text top="844" left="81" width="359" height="13" font="3">algorithms. In essence, there is a coordination cost to be</text>
<text top="860" left="81" width="359" height="13" font="3">paid for load balancing, as thread synchronization is neces-</text>
<text top="875" left="81" width="359" height="13" font="3">sary. Skew in this case causes contention, stressing the cache</text>
<text top="891" left="81" width="359" height="13" font="3">coherence protocol and increasing memory traﬃc. On the</text>
<text top="907" left="81" width="359" height="13" font="3">other hand, the no partitioning algorithm does skewed mem-</text>
<text top="922" left="81" width="359" height="13" font="3">ory loads of read-only data, which is handled very eﬃciently</text>
<text top="938" left="81" width="210" height="13" font="3">by modern CPUs through caching.</text>
<text top="981" left="81" width="22" height="16" font="1">4.8</text>
<text top="981" left="121" width="242" height="16" font="1">Effect of output materialization</text>
<text top="1003" left="94" width="345" height="13" font="3">Early work in main memory join processing [7] did not</text>
<text top="1019" left="81" width="358" height="13" font="3">take into account the cost of materialization. This decision</text>
<text top="1034" left="81" width="359" height="13" font="3">was justiﬁed by pointing out that materialization comes at a</text>
<text top="1050" left="81" width="359" height="13" font="3">ﬁxed price for all algorithms and, therefore, a join algorithm</text>
<text top="1066" left="81" width="359" height="13" font="3">will be faster regardless of the output being materialized or</text>
<text top="398" left="534" width="59" height="13" font="3">Machine</text>
<text top="398" left="644" width="24" height="13" font="3">NO</text>
<text top="398" left="687" width="21" height="13" font="3">SN</text>
<text top="398" left="726" width="32" height="13" font="3">L2-S</text>
<text top="398" left="774" width="35" height="13" font="3">L2-R</text>
<text top="414" left="521" width="85" height="13" font="3">Intel Nehalem</text>
<text top="414" left="644" width="25" height="13" font="3">23%</text>
<text top="414" left="688" width="18" height="13" font="3">4%</text>
<text top="414" left="733" width="18" height="13" font="3">7%</text>
<text top="414" left="779" width="25" height="13" font="3">10%</text>
<text top="431" left="500" width="127" height="13" font="3">Sun UltraSPARC T2</text>
<text top="431" left="644" width="25" height="13" font="3">29%</text>
<text top="431" left="685" width="25" height="13" font="3">21%</text>
<text top="431" left="730" width="25" height="13" font="3">20%</text>
<text top="431" left="779" width="25" height="13" font="3">23%</text>
<text top="462" left="475" width="359" height="13" font="3">Table 7: Additional overhead of materialization with</text>
<text top="477" left="475" width="359" height="13" font="3">respect to total cycles without materialization on</text>
<text top="493" left="475" width="142" height="13" font="3">the uniform dataset.</text>
<text top="510" left="550" width="61" height="13" font="3">Scale 0.5</text>
<text top="510" left="656" width="49" height="13" font="3">Scale 1</text>
<text top="510" left="753" width="49" height="13" font="3">Scale 2</text>
<text top="526" left="495" width="21" height="13" font="3">NO</text>
<text top="526" left="548" width="75" height="13" font="3">7.65 (0.47X)</text>
<text top="526" left="639" width="82" height="13" font="3">16.15 (1.00X)</text>
<text top="526" left="737" width="82" height="13" font="3">62.27 (3.86X)</text>
<text top="543" left="497" width="18" height="13" font="3">SN</text>
<text top="543" left="541" width="82" height="13" font="3">11.76 (0.46X)</text>
<text top="543" left="639" width="82" height="13" font="3">25.62 (1.00X)</text>
<text top="543" left="737" width="82" height="13" font="3">98.82 (3.86X)</text>
<text top="559" left="492" width="28" height="13" font="3">L2-S</text>
<text top="559" left="548" width="75" height="13" font="3">8.47 (0.47X)</text>
<text top="559" left="639" width="82" height="13" font="3">18.13 (1.00X)</text>
<text top="559" left="737" width="82" height="13" font="3">68.48 (3.78X)</text>
<text top="575" left="490" width="30" height="13" font="3">L2-R</text>
<text top="575" left="548" width="75" height="13" font="3">5.82 (0.46X)</text>
<text top="575" left="639" width="82" height="13" font="3">12.71 (1.00X)</text>
<text top="575" left="763" width="30" height="13" font="3">DNF</text>
<text top="606" left="475" width="359" height="13" font="3">Table 8: Join sensitivity with varying input cardi-</text>
<text top="622" left="475" width="359" height="13" font="3">nalities for the uniform dataset on Intel Nehalem.</text>
<text top="638" left="475" width="359" height="13" font="3">The table shows the cycles for computing the join</text>
<text top="654" left="475" width="343" height="13" font="3">(in billions) and the relative diﬀerence to scale 1.</text>
<text top="687" left="475" width="359" height="13" font="3">discarded. Recent work by Cieslewicz et al. [3] highlighted</text>
<text top="703" left="475" width="326" height="13" font="3">the trade-oﬀs involved when materializing the output.</text>
<text top="719" left="489" width="345" height="13" font="3">In Table 7 we report the increase in the total join comple-</text>
<text top="734" left="475" width="359" height="13" font="3">tion time when we materialize the output in memory for the</text>
<text top="750" left="475" width="359" height="13" font="3">uniform dataset and the partitioning strategies described in</text>
<text top="766" left="475" width="359" height="13" font="3">Table 2. If the join operator is part of a complex query</text>
<text top="781" left="475" width="359" height="13" font="3">plan, it is unlikely that the entire join output will ever need</text>
<text top="797" left="475" width="359" height="13" font="3">to be written in one big memory block, but, even in this</text>
<text top="813" left="475" width="359" height="13" font="3">extreme case, we see that no algorithm is being signiﬁcantly</text>
<text top="828" left="475" width="173" height="13" font="3">impacted by materialization.</text>
<text top="856" left="475" width="22" height="16" font="1">4.9</text>
<text top="856" left="516" width="187" height="16" font="1">Cardinality experiments</text>
<text top="878" left="489" width="345" height="13" font="3">We now explore how sensitive our ﬁndings are to varia-</text>
<text top="893" left="475" width="359" height="13" font="3">tions in the cardinalities of the two input relations. Table 8</text>
<text top="909" left="475" width="359" height="13" font="3">shows the results when running the join algorithms on the</text>
<text top="925" left="475" width="359" height="13" font="3">Intel Nehalem machine. The numbers obtained from the uni-</text>
<text top="940" left="475" width="359" height="13" font="3">form dataset (described in detail in Section 4.1) are shown</text>
<text top="956" left="475" width="359" height="13" font="3">in the middle column. We ﬁrst created one uniform dataset</text>
<text top="972" left="475" width="359" height="13" font="3">where both relations are half the size (scale 0.5). This means</text>
<text top="987" left="475" width="359" height="13" font="3">the relation R has 8M tuples and the relation S has 128M tu-</text>
<text top="1003" left="475" width="359" height="13" font="3">ples. We also created a uniform dataset where both relations</text>
<text top="1019" left="475" width="359" height="13" font="3">are twice the size (scale 2), i.e. the relation R has 32M tu-</text>
<text top="1034" left="475" width="359" height="13" font="3">ples and the relation S has 512M tuples. The scale 2 dataset</text>
<text top="1050" left="475" width="359" height="13" font="3">occupies 9GB out of the 12GB of memory our system has</text>
<text top="1066" left="475" width="358" height="13" font="3">(Table 1) and leaves little working memory, but the serial</text>
</page>
<page number="11" position="absolute" top="0" left="0" height="1188" width="918">
<text top="263" left="129" width="5" height="8" font="8">0</text>
<text top="240" left="120" width="14" height="8" font="8">100</text>
<text top="217" left="120" width="14" height="8" font="8">200</text>
<text top="194" left="120" width="14" height="8" font="8">300</text>
<text top="172" left="120" width="14" height="8" font="8">400</text>
<text top="149" left="120" width="14" height="8" font="8">500</text>
<text top="126" left="120" width="14" height="8" font="8">600</text>
<text top="104" left="120" width="14" height="8" font="8">700</text>
<text top="81" left="120" width="14" height="8" font="8">800</text>
<text top="269" left="153" width="0" height="8" font="8">1</text>
<text top="274" left="180" width="7" height="8" font="8">16 64</text>
<text top="278" left="193" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="240" width="0" height="8" font="8">12</text>
<text top="275" left="240" width="0" height="8" font="8">8K</text>
<text top="274" left="266" width="7" height="8" font="8">16 64</text>
<text top="278" left="280" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="326" width="0" height="8" font="8">12</text>
<text top="275" left="326" width="0" height="8" font="8">8K</text>
<text top="274" left="353" width="7" height="8" font="8">16 64</text>
<text top="278" left="366" width="40" height="8" font="8">256 512 1K 2K 4K 8K 32K</text>
<text top="284" left="412" width="0" height="8" font="8">12</text>
<text top="275" left="412" width="0" height="8" font="8">8K</text>
<text top="211" left="108" width="0" height="8" font="8">Cycl</text>
<text top="195" left="108" width="0" height="8" font="8">es pe</text>
<text top="175" left="108" width="0" height="8" font="8">r </text>
<text top="170" left="108" width="0" height="8" font="8">o</text>
<text top="166" left="108" width="0" height="8" font="8">utpu</text>
<text top="150" left="108" width="0" height="8" font="8">t t</text>
<text top="143" left="108" width="0" height="8" font="8">u</text>
<text top="139" left="108" width="0" height="8" font="8">pl</text>
<text top="132" left="108" width="0" height="8" font="8">e</text>
<text top="324" left="241" width="73" height="8" font="8">Number of partitions</text>
<text top="307" left="354" width="39" height="8" font="8">Radix-best</text>
<text top="307" left="265" width="45" height="8" font="8">Independent</text>
<text top="307" left="188" width="26" height="8" font="8">Shared</text>
<text top="307" left="142" width="10" height="8" font="8">No</text>
<text top="86" left="206" width="38" height="10" font="9">partition</text>
<text top="87" left="292" width="23" height="10" font="9">build</text>
<text top="86" left="361" width="28" height="10" font="9">probe</text>
<text top="353" left="81" width="359" height="13" font="3">Figure 6: Experiment on Intel Nehalem with uni-</text>
<text top="369" left="81" width="179" height="13" font="3">form dataset and |R|=|S|.</text>
<text top="403" left="81" width="359" height="13" font="3">access pattern allows performance to degrade gracefully for</text>
<text top="419" left="81" width="359" height="13" font="3">all algorithms but the L2-R algorithm. The main memory</text>
<text top="435" left="81" width="359" height="13" font="3">optimizations of the L2-R algorithm cause many random ac-</text>
<text top="451" left="81" width="359" height="13" font="3">cesses which hurt performance. We therefore mark the L2-R</text>
<text top="466" left="81" width="200" height="13" font="3">algorithm as not ﬁnished (DNF).</text>
<text top="482" left="94" width="345" height="13" font="3">We now examine the impact of the relative size of the rela-</text>
<text top="498" left="81" width="359" height="13" font="3">tions R and S. We ﬁxed the cardinality of the relation S to</text>
<text top="513" left="81" width="359" height="13" font="3">be 16M tuples, making |R| = |S|, and we plot the cycles per</text>
<text top="529" left="81" width="359" height="13" font="3">output tuple for the uniform dataset when running on the</text>
<text top="545" left="81" width="359" height="13" font="3">Intel Nehalem in Figure 6. First, the partitioning time in-</text>
<text top="560" left="81" width="359" height="13" font="3">creases proportionally to |R| + |S|. Second, the build phase</text>
<text top="576" left="81" width="358" height="13" font="3">becomes signiﬁcant, taking at least 25% of the total join</text>
<text top="592" left="81" width="359" height="13" font="3">completion time. The probe phase, however, is at most 30%</text>
<text top="607" left="81" width="359" height="13" font="3">slower, and less aﬀected by the cardinality of the relation R.</text>
<text top="623" left="81" width="359" height="13" font="3">Overall, all the algorithms are slower when |R| = |S| because</text>
<text top="639" left="81" width="358" height="13" font="3">they have to process more data, but the no partitioning algo-</text>
<text top="654" left="81" width="359" height="13" font="3">rithm is slightly favored because it avoids partitioning both</text>
<text top="670" left="81" width="92" height="13" font="3">input relations.</text>
<text top="686" left="94" width="345" height="13" font="3">The results show that no join algorithm is particularly sen-</text>
<text top="702" left="81" width="359" height="13" font="3">sitive to our selection of input relation cardinalities, there-</text>
<text top="717" left="81" width="359" height="13" font="3">fore our ﬁndings are expected to hold across a broader spec-</text>
<text top="733" left="81" width="359" height="13" font="3">trum of cardinalities. The outcome of the experiments for</text>
<text top="749" left="81" width="310" height="13" font="3">the Sun UltraSPARC T2 is similar, and is omitted.</text>
<text top="777" left="81" width="31" height="16" font="1">4.10</text>
<text top="777" left="130" width="169" height="16" font="1">Selectivity experiment</text>
<text top="799" left="94" width="345" height="13" font="3">We now turn our attention to how join selectivity aﬀects</text>
<text top="815" left="81" width="359" height="13" font="3">performance. As all our original datasets are examples of</text>
<text top="831" left="81" width="359" height="13" font="3">joins between primary and foreign keys, all the experiments</text>
<text top="846" left="81" width="359" height="13" font="3">that have been presented so far have a selectivity of 100%.</text>
<text top="862" left="81" width="359" height="13" font="3">For this experiment we created two diﬀerent S relations that</text>
<text top="878" left="81" width="359" height="13" font="3">have the same cardinality but only 50% and 12.5% of the tu-</text>
<text top="893" left="81" width="359" height="13" font="3">ples join with a tuple in the relation R. The key distribution</text>
<text top="909" left="81" width="65" height="13" font="3">is uniform.</text>
<text top="925" left="94" width="345" height="13" font="3">Results for the Intel Nehalem are shown Figure 7(a). De-</text>
<text top="940" left="81" width="359" height="13" font="3">creasing join selectivity has a marginal beneﬁt on the probe</text>
<text top="956" left="81" width="359" height="13" font="3">phase, but the other two phases are unaﬀected. The out-</text>
<text top="972" left="81" width="359" height="13" font="3">come of the same experiment on Sun UltraSPARC T2 is</text>
<text top="987" left="81" width="359" height="13" font="3">shown in Figure 7(b). In this architecture, the beneﬁt of a</text>
<text top="1003" left="81" width="327" height="13" font="3">small join selectivity on the probe phase is signiﬁcant.</text>
<text top="1019" left="94" width="345" height="13" font="3">Inspecting the performance counters in this experiment</text>
<text top="1034" left="81" width="359" height="13" font="3">revealed additional insights. Across all the architectures,</text>
<text top="1050" left="81" width="359" height="13" font="3">the code path length (i.e. instructions executed) increases as</text>
<text top="1066" left="81" width="358" height="13" font="3">join selectivity increases. The Intel Nehalem is practically</text>
<text top="85" left="475" width="359" height="13" font="3">insensitive to diﬀerent join selectivities, because its out-of-</text>
<text top="101" left="475" width="358" height="13" font="3">order execution manages to overlap the data transfer with</text>
<text top="117" left="475" width="359" height="13" font="3">the byte shuﬄing that is required to assemble the output</text>
<text top="133" left="475" width="359" height="13" font="3">tuple. On the other hand, for the Sun UltraSPARC T2 ma-</text>
<text top="148" left="475" width="359" height="13" font="3">chine, there is a strong linear correlation between the code</text>
<text top="164" left="475" width="359" height="13" font="3">path length and the cycles that are required for the probe</text>
<text top="180" left="475" width="359" height="13" font="3">phase to complete. The in-order Sun UltraSPARC T2 can-</text>
<text top="195" left="475" width="359" height="13" font="3">not automatically extract the instruction-level parallelism of</text>
<text top="211" left="475" width="359" height="13" font="3">the probe phase, unless the programmer explicitly expresses</text>
<text top="227" left="475" width="172" height="13" font="3">it by using multiple threads.</text>
<text top="253" left="475" width="31" height="16" font="1">4.11</text>
<text top="253" left="525" width="96" height="16" font="1">Implications</text>
<text top="275" left="489" width="345" height="13" font="3">These results imply that DBMSs must reconsider their</text>
<text top="291" left="475" width="358" height="13" font="3">join algorithms for current and future multi-core processors.</text>
<text top="307" left="475" width="359" height="13" font="3">First, modern processors are very eﬀective in hiding cache</text>
<text top="322" left="475" width="359" height="13" font="3">miss latencies through multi-threading (SMT), as it is shown</text>
<text top="338" left="475" width="359" height="13" font="3">in Tables 5 and 6. Second, optimizing for cache performance</text>
<text top="354" left="475" width="359" height="13" font="3">requires partitioning, and this has additional computation</text>
<text top="369" left="475" width="359" height="13" font="3">and synchronization overheads, and necessitates elaborate</text>
<text top="385" left="475" width="359" height="13" font="3">load balancing techniques to deal with skew. These costs of</text>
<text top="401" left="475" width="359" height="13" font="3">partitioning on a modern multi-core machine can be higher</text>
<text top="416" left="475" width="359" height="13" font="3">than the beneﬁt of an increased cache hit rate, especially on</text>
<text top="432" left="475" width="358" height="13" font="3">skewed datasets (as shown in Figures 2 and 3.) To fully</text>
<text top="448" left="475" width="359" height="13" font="3">leverage the current and future CPUs, high performance</text>
<text top="463" left="475" width="359" height="13" font="3">main memory designs have to achieve good cache and TLB</text>
<text top="479" left="475" width="359" height="13" font="3">performance, while fully exploiting SMT, and minimizing</text>
<text top="495" left="475" width="132" height="13" font="3">synchronization costs.</text>
<text top="531" left="475" width="13" height="16" font="1">5.</text>
<text top="531" left="507" width="148" height="16" font="1">RELATED WORK</text>
<text top="553" left="489" width="345" height="13" font="3">There is a rich history of studying hash join performance</text>
<text top="569" left="475" width="358" height="13" font="3">for main memory database systems, starting with the early</text>
<text top="584" left="475" width="358" height="13" font="3">work of DeWitt et al. [7]. A decade later Shatdal et al. [16]</text>
<text top="600" left="475" width="359" height="13" font="3">studied cache-conscious algorithms for query execution and</text>
<text top="616" left="475" width="359" height="13" font="3">discovered that the probe phase dominates the overall hash</text>
<text top="631" left="475" width="359" height="13" font="3">join processing time. They also showed that hash join com-</text>
<text top="647" left="475" width="359" height="13" font="3">putation can be sped up if both the build and probe relations</text>
<text top="663" left="475" width="239" height="13" font="3">are partitioned so as to ﬁt in the cache.</text>
<text top="678" left="489" width="345" height="13" font="3">Ailamaki et al. [1] studied where DBMSs spend their time</text>
<text top="694" left="475" width="360" height="13" font="3">on modern processors, whereas Manegold et al. [12] inspected</text>
<text top="710" left="475" width="359" height="13" font="3">the time breakdown for a hash join operation. Both papers</text>
<text top="725" left="475" width="358" height="13" font="3">break down the query execution time by examining perfor-</text>
<text top="741" left="475" width="359" height="13" font="3">mance counters, and single out cache and TLB misses as the</text>
<text top="757" left="475" width="358" height="13" font="3">two primary culprits for suboptimal performance in main</text>
<text top="773" left="475" width="359" height="13" font="3">memory processing. A follow-up paper [13] presented a cost</text>
<text top="788" left="475" width="358" height="13" font="3">model on how to optimize the performance of the radix join</text>
<text top="804" left="475" width="191" height="13" font="3">algorithm on a uniprocessor [2].</text>
<text top="820" left="489" width="345" height="13" font="3">Ross [15] presented a more eﬃcient way to improve the</text>
<text top="835" left="475" width="359" height="13" font="3">performance of hash joins by using cuckoo hashing [14] and</text>
<text top="851" left="475" width="359" height="13" font="3">SIMD instructions. Garcia and Korth [9] have studied the</text>
<text top="867" left="475" width="358" height="13" font="3">beneﬁts of using simultaneous multi-threading for hash join</text>
<text top="882" left="475" width="358" height="13" font="3">processing. Graefe et al. [10] described how hash-based algo-</text>
<text top="898" left="475" width="359" height="13" font="3">rithms can improve the performance of a commercial DBMS.</text>
<text top="914" left="489" width="345" height="13" font="3">Finally, there has been prior work in handling skew during</text>
<text top="930" left="475" width="359" height="13" font="3">hash join processing. The experiments with a high number</text>
<text top="945" left="475" width="359" height="13" font="3">of partitions that we presented in Section 4.4 are an exten-</text>
<text top="961" left="475" width="359" height="13" font="3">sion of an idea by DeWitt et al. [8] for a main memory,</text>
<text top="976" left="475" width="145" height="13" font="3">multi-core environment.</text>
<text top="1013" left="475" width="13" height="16" font="1">6.</text>
<text top="1013" left="507" width="316" height="16" font="1">CONCLUSIONS AND FUTURE WORK</text>
<text top="1034" left="489" width="345" height="13" font="3">The rapidly evolving multi-core landscape requires that</text>
<text top="1050" left="475" width="359" height="13" font="3">DBMSs carefully consider the interactions between query</text>
<text top="1066" left="475" width="359" height="13" font="3">processing algorithms and the underlying hardware. In this</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="10" size="5" family="Times" color="#000000"/>
	<fontspec id="11" size="7" family="Times" color="#000000"/>
<text top="242" left="167" width="4" height="7" font="10">0</text>
<text top="226" left="163" width="8" height="7" font="10">10</text>
<text top="210" left="163" width="8" height="7" font="10">20</text>
<text top="194" left="163" width="8" height="7" font="10">30</text>
<text top="178" left="163" width="8" height="7" font="10">40</text>
<text top="162" left="163" width="8" height="7" font="10">50</text>
<text top="145" left="163" width="8" height="7" font="10">60</text>
<text top="129" left="163" width="8" height="7" font="10">70</text>
<text top="113" left="163" width="8" height="7" font="10">80</text>
<text top="97" left="163" width="8" height="7" font="10">90</text>
<text top="81" left="159" width="12" height="7" font="10">100</text>
<text top="255" left="205" width="0" height="7" font="10">NO</text>
<text top="254" left="218" width="0" height="7" font="10">SN</text>
<text top="259" left="232" width="0" height="7" font="10">L2</text>
<text top="251" left="232" width="0" height="7" font="10">-S</text>
<text top="260" left="246" width="0" height="7" font="10">L2</text>
<text top="252" left="246" width="0" height="7" font="10">-R</text>
<text top="255" left="287" width="0" height="7" font="10">NO</text>
<text top="254" left="301" width="0" height="7" font="10">SN</text>
<text top="259" left="315" width="0" height="7" font="10">L2</text>
<text top="251" left="315" width="0" height="7" font="10">-S</text>
<text top="260" left="329" width="0" height="7" font="10">L2</text>
<text top="252" left="329" width="0" height="7" font="10">-R</text>
<text top="255" left="370" width="0" height="7" font="10">NO</text>
<text top="254" left="384" width="0" height="7" font="10">SN</text>
<text top="259" left="398" width="0" height="7" font="10">L2</text>
<text top="251" left="398" width="0" height="7" font="10">-S</text>
<text top="260" left="411" width="0" height="7" font="10">L2</text>
<text top="252" left="411" width="0" height="7" font="10">-R</text>
<text top="196" left="149" width="0" height="7" font="10">Cycl</text>
<text top="182" left="149" width="0" height="7" font="10">es pe</text>
<text top="165" left="149" width="0" height="7" font="10">r </text>
<text top="160" left="149" width="0" height="7" font="10">o</text>
<text top="156" left="149" width="0" height="7" font="10">utpu</text>
<text top="142" left="149" width="0" height="7" font="10">t t</text>
<text top="136" left="149" width="0" height="7" font="10">u</text>
<text top="132" left="149" width="0" height="7" font="10">pl</text>
<text top="127" left="149" width="0" height="7" font="10">e</text>
<text top="297" left="276" width="46" height="7" font="10">Join selectivity</text>
<text top="282" left="373" width="18" height="7" font="10">100%</text>
<text top="282" left="292" width="14" height="7" font="10">50%</text>
<text top="282" left="209" width="14" height="7" font="10">12%</text>
<text top="85" left="226" width="34" height="9" font="11">partition</text>
<text top="86" left="302" width="20" height="9" font="11">build</text>
<text top="85" left="364" width="25" height="9" font="11">probe</text>
<text top="310" left="230" width="107" height="13" font="3">(a) Intel Nehalem</text>
<text top="242" left="514" width="4" height="7" font="10">0</text>
<text top="226" left="510" width="8" height="7" font="10">10</text>
<text top="210" left="510" width="8" height="7" font="10">20</text>
<text top="194" left="510" width="8" height="7" font="10">30</text>
<text top="178" left="510" width="8" height="7" font="10">40</text>
<text top="162" left="510" width="8" height="7" font="10">50</text>
<text top="145" left="510" width="8" height="7" font="10">60</text>
<text top="129" left="510" width="8" height="7" font="10">70</text>
<text top="113" left="510" width="8" height="7" font="10">80</text>
<text top="97" left="510" width="8" height="7" font="10">90</text>
<text top="81" left="506" width="12" height="7" font="10">100</text>
<text top="255" left="552" width="0" height="7" font="10">NO</text>
<text top="254" left="566" width="0" height="7" font="10">SN</text>
<text top="259" left="579" width="0" height="7" font="10">L2</text>
<text top="251" left="579" width="0" height="7" font="10">-S</text>
<text top="260" left="593" width="0" height="7" font="10">L2</text>
<text top="252" left="593" width="0" height="7" font="10">-R</text>
<text top="255" left="635" width="0" height="7" font="10">NO</text>
<text top="254" left="648" width="0" height="7" font="10">SN</text>
<text top="259" left="662" width="0" height="7" font="10">L2</text>
<text top="251" left="662" width="0" height="7" font="10">-S</text>
<text top="260" left="676" width="0" height="7" font="10">L2</text>
<text top="252" left="676" width="0" height="7" font="10">-R</text>
<text top="255" left="717" width="0" height="7" font="10">NO</text>
<text top="254" left="731" width="0" height="7" font="10">SN</text>
<text top="259" left="745" width="0" height="7" font="10">L2</text>
<text top="251" left="745" width="0" height="7" font="10">-S</text>
<text top="260" left="759" width="0" height="7" font="10">L2</text>
<text top="252" left="759" width="0" height="7" font="10">-R</text>
<text top="196" left="496" width="0" height="7" font="10">Cycl</text>
<text top="182" left="496" width="0" height="7" font="10">es pe</text>
<text top="165" left="496" width="0" height="7" font="10">r </text>
<text top="160" left="496" width="0" height="7" font="10">o</text>
<text top="156" left="496" width="0" height="7" font="10">utpu</text>
<text top="142" left="496" width="0" height="7" font="10">t t</text>
<text top="136" left="496" width="0" height="7" font="10">u</text>
<text top="132" left="496" width="0" height="7" font="10">pl</text>
<text top="127" left="496" width="0" height="7" font="10">e</text>
<text top="297" left="623" width="46" height="7" font="10">Join selectivity</text>
<text top="282" left="720" width="18" height="7" font="10">100%</text>
<text top="282" left="639" width="14" height="7" font="10">50%</text>
<text top="282" left="556" width="14" height="7" font="10">12%</text>
<text top="85" left="573" width="34" height="9" font="11">partition</text>
<text top="86" left="649" width="20" height="9" font="11">build</text>
<text top="85" left="711" width="25" height="9" font="11">probe</text>
<text top="310" left="556" width="150" height="13" font="3">(b) Sun UltraSPARC T2</text>
<text top="333" left="81" width="753" height="13" font="3">Figure 7: Sensitivity to join selectivity. Increasing join selectivity impacts the critical path for the Sun</text>
<text top="348" left="81" width="753" height="13" font="3">UltraSPARC T2, while the out-of-order execution on Intel Nehalem overlaps computation with data transfer.</text>
<text top="386" left="81" width="359" height="13" font="3">paper we examine these interactions when executing a hash</text>
<text top="402" left="81" width="359" height="13" font="3">join operation in a main memory DBMS. We implement</text>
<text top="417" left="81" width="359" height="13" font="3">a family of main memory hash join algorithms that vary in</text>
<text top="433" left="81" width="359" height="13" font="3">the way that they implement the partition, build, and probe</text>
<text top="449" left="81" width="253" height="13" font="3">phases of a canonical hash join algorithm.</text>
<text top="464" left="94" width="345" height="13" font="3">We also evaluate our algorithms on two diﬀerent multi-</text>
<text top="480" left="81" width="359" height="13" font="3">core processor architectures. Our results show that a simple</text>
<text top="496" left="81" width="359" height="13" font="3">hash join technique that does not do any partitioning of the</text>
<text top="512" left="81" width="359" height="13" font="3">input relations often outperforms the other more complex</text>
<text top="527" left="81" width="359" height="13" font="3">partitioning-based join alternatives. In addition, the relative</text>
<text top="543" left="81" width="359" height="13" font="3">performance of this simple hash join technique rapidly im-</text>
<text top="559" left="81" width="359" height="13" font="3">proves with increasing skew, and it outperforms every other</text>
<text top="574" left="81" width="345" height="13" font="3">algorithm in the presence of even small amounts of skew.</text>
<text top="590" left="94" width="345" height="13" font="3">Minimizing cache misses requires additional computation,</text>
<text top="606" left="81" width="359" height="13" font="3">synchronization and load balancing to cope with skew. As</text>
<text top="621" left="81" width="359" height="13" font="3">our experiments show, these costs on a modern multi-core</text>
<text top="637" left="81" width="359" height="13" font="3">machine can be higher than the beneﬁt of an increased cache</text>
<text top="653" left="81" width="359" height="13" font="3">hit rate. To fully leverage the current and future CPUs, high</text>
<text top="669" left="81" width="359" height="13" font="3">performance main memory designs have to consider how to</text>
<text top="684" left="81" width="359" height="13" font="3">minimize computation and synchronization costs, and fully</text>
<text top="700" left="81" width="359" height="13" font="3">exploit simultaneous multi-threading, in addition to main-</text>
<text top="715" left="81" width="359" height="13" font="3">taining good cache and TLB behavior. While a large part of</text>
<text top="731" left="81" width="359" height="13" font="3">the previous work in this area has mostly focused on mini-</text>
<text top="747" left="81" width="359" height="13" font="3">mizing cache and TLB misses for database query processing</text>
<text top="763" left="81" width="359" height="13" font="3">tasks, our work here suggests that paying attention to the</text>
<text top="778" left="81" width="359" height="13" font="3">computation and synchronization costs is also very impor-</text>
<text top="794" left="81" width="359" height="13" font="3">tant in modern processors. This work points to a rich direc-</text>
<text top="810" left="81" width="359" height="13" font="3">tion for future work in exploring the design of more complex</text>
<text top="825" left="81" width="359" height="13" font="3">query processing techniques (beyond single joins) that con-</text>
<text top="841" left="81" width="359" height="13" font="3">sider the joint impact of computation, synchronization costs,</text>
<text top="857" left="81" width="216" height="13" font="3">load balancing, and cache behavior.</text>
<text top="900" left="81" width="140" height="16" font="1">Acknowledgments</text>
<text top="924" left="81" width="359" height="13" font="3">We thank David DeWitt for his deeply insightful comments</text>
<text top="940" left="81" width="359" height="13" font="3">on this paper. We also thank the reviewers of this paper</text>
<text top="955" left="81" width="359" height="13" font="3">and Willis Lang for their feedback on an earlier draft of this</text>
<text top="971" left="81" width="359" height="13" font="3">paper. David Wood and the Wisconsin Multifacet project</text>
<text top="987" left="81" width="359" height="13" font="3">were invaluable supporters of this project and gave us ex-</text>
<text top="1002" left="81" width="359" height="13" font="3">clusive access to their hardware, and we thank them. This</text>
<text top="1018" left="81" width="358" height="13" font="3">work was supported in part by a grant from the Microsoft</text>
<text top="1034" left="81" width="359" height="13" font="3">Jim Gray Systems Lab, and in part by the National Science</text>
<text top="1049" left="81" width="341" height="13" font="3">Foundation under grants IIS-0963993 and CNS-0551401.</text>
<text top="384" left="475" width="13" height="16" font="1">7.</text>
<text top="384" left="507" width="120" height="16" font="1">REFERENCES</text>
<text top="410" left="482" width="336" height="12" font="4">[1] A. Ailamaki, D. J. DeWitt, M. D. Hill, and D. A. Wood.</text>
<text top="423" left="502" width="316" height="12" font="4">DBMSs on a modern processor: Where does time go? In</text>
<text top="437" left="502" width="36" height="11" font="4">VLDB</text>
<text top="437" left="538" width="123" height="12" font="4">, pages 266–277, 1999.</text>
<text top="451" left="481" width="333" height="12" font="4">[2] P. A. Boncz, S. Manegold, and M. L. Kersten. Database</text>
<text top="465" left="502" width="309" height="12" font="4">architecture optimized for the new bottleneck: Memory</text>
<text top="478" left="502" width="204" height="12" font="4">access. In VLDB, pages 54–65, 1999.</text>
<text top="493" left="481" width="332" height="12" font="4">[3] J. Cieslewicz, W. Mee, and K. A. Ross. Cache-conscious</text>
<text top="507" left="502" width="308" height="12" font="4">buﬀering for database operators with state. In DaMoN,</text>
<text top="520" left="502" width="103" height="12" font="4">pages 43–51, 2009.</text>
<text top="535" left="482" width="330" height="12" font="4">[4] J. Cieslewicz and K. A. Ross. Data partitioning on chip</text>
<text top="548" left="502" width="264" height="12" font="4">multiprocessors. In DaMoN, pages 25–34, 2008.</text>
<text top="563" left="481" width="322" height="12" font="4">[5] J. Cieslewicz, K. A. Ross, and I. Giannakakis. Parallel</text>
<text top="577" left="502" width="277" height="12" font="4">buﬀers for chip multiprocessors. In DaMoN, 2007.</text>
<text top="592" left="481" width="344" height="12" font="4">[6] D. J. DeWitt and J. Gray. Parallel database systems: The</text>
<text top="605" left="502" width="319" height="12" font="4">future of database processing or a passing fad? SIGMOD</text>
<text top="619" left="502" width="37" height="11" font="4">Record</text>
<text top="619" left="538" width="121" height="12" font="4">, 19(4):104–112, 1990.</text>
<text top="634" left="481" width="306" height="12" font="4">[7] D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro,</text>
<text top="647" left="502" width="280" height="12" font="4">M. Stonebraker, and D. A. Wood. Implementation</text>
<text top="661" left="502" width="278" height="12" font="4">techniques for main memory database systems. In</text>
<text top="675" left="502" width="119" height="11" font="4">SIGMOD Conference</text>
<text top="674" left="621" width="98" height="12" font="4">, pages 1–8, 1984.</text>
<text top="689" left="481" width="310" height="12" font="4">[8] D. J. DeWitt, J. F. Naughton, D. A. Schneider, and</text>
<text top="702" left="502" width="311" height="12" font="4">S. Seshadri. Practical skew handling in parallel joins. In</text>
<text top="716" left="502" width="36" height="11" font="4">VLDB</text>
<text top="716" left="538" width="110" height="12" font="4">, pages 27–40, 1992.</text>
<text top="731" left="481" width="344" height="12" font="4">[9] P. Garcia and H. F. Korth. Database hash-join algorithms</text>
<text top="744" left="502" width="282" height="12" font="4">on multithreaded computer architectures. In Conf.</text>
<text top="758" left="502" width="116" height="11" font="4">Computing Frontiers</text>
<text top="758" left="618" width="123" height="12" font="4">, pages 241–252, 2006.</text>
<text top="773" left="475" width="353" height="12" font="4">[10] G. Graefe, R. Bunker, and S. Cooper. Hash joins and hash</text>
<text top="786" left="502" width="309" height="12" font="4">teams in Microsoft SQL Server. In VLDB, pages 86–97,</text>
<text top="800" left="502" width="29" height="12" font="4">1998.</text>
<text top="815" left="475" width="317" height="12" font="4">[11] C. Kim, E. Sedlar, J. Chhugani, T. Kaldewey, A. D.</text>
<text top="828" left="502" width="319" height="12" font="4">Nguyen, A. D. Blas, V. W. Lee, N. Satish, and P. Dubey.</text>
<text top="841" left="502" width="290" height="12" font="4">Sort vs. hash revisited: Fast join implementation on</text>
<text top="855" left="502" width="319" height="12" font="4">modern multi-core CPUs. PVLDB, 2(2):1378–1389, 2009.</text>
<text top="870" left="475" width="319" height="12" font="4">[12] S. Manegold, P. A. Boncz, and M. L. Kersten. What</text>
<text top="883" left="502" width="295" height="12" font="4">happens during a join? Dissecting CPU and memory</text>
<text top="897" left="502" width="292" height="12" font="4">optimization eﬀects. In VLDB, pages 339–350, 2000.</text>
<text top="912" left="475" width="350" height="12" font="4">[13] S. Manegold, P. A. Boncz, and M. L. Kersten. Optimizing</text>
<text top="925" left="502" width="300" height="12" font="4">main-memory join on modern hardware. IEEE Trans.</text>
<text top="939" left="502" width="102" height="11" font="4">Knowl. Data Eng.</text>
<text top="939" left="604" width="121" height="12" font="4">, 14(4):709–730, 2002.</text>
<text top="953" left="475" width="352" height="12" font="4">[14] R. Pagh and F. F. Rodler. Cuckoo hashing. J. Algorithms,</text>
<text top="967" left="502" width="113" height="12" font="4">51(2):122–144, 2004.</text>
<text top="982" left="475" width="354" height="12" font="4">[15] K. A. Ross. Eﬃcient hash probes on modern processors. In</text>
<text top="996" left="502" width="32" height="11" font="4">ICDE</text>
<text top="995" left="534" width="136" height="12" font="4">, pages 1297–1301, 2007.</text>
<text top="1010" left="475" width="353" height="12" font="4">[16] A. Shatdal, C. Kant, and J. F. Naughton. Cache conscious</text>
<text top="1024" left="502" width="329" height="12" font="4">algorithms for relational query processing. In VLDB, pages</text>
<text top="1037" left="502" width="81" height="12" font="4">510–521, 1994.</text>
<text top="1052" left="475" width="337" height="12" font="4">[17] M. Stonebraker. The case for shared nothing. In HPTS,</text>
<text top="1066" left="502" width="29" height="12" font="4">1985.</text>
</page>
</pdf2xml>
